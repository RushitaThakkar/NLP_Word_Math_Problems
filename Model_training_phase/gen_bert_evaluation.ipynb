{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gen_bert1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7GAb9gK52Tj",
        "outputId": "3c40bf79-3025-4187-fc5b-1a1a519f83d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "%cd \"/content/gen_bert\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gen_bert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnt4wmJ57KD-"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Gw9Z7xf7jYW",
        "outputId": "2be48489-d111-4d81-d2aa-06fb5ae4b4c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!git clone https://github.com/ag1988/injecting_numeracy.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'injecting_numeracy'...\n",
            "remote: Enumerating objects: 130, done.\u001b[K\n",
            "remote: Counting objects: 100% (130/130), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 130 (delta 49), reused 89 (delta 27), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (130/130), 103.54 KiB | 4.14 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYjK3sp_7nW0",
        "outputId": "1cea0ce4-6e85-4041-f7f0-dd7edf0fa2d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "%cd /content/gen_bert/injecting_numeracy/pre_training"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gen_bert/injecting_numeracy/pre_training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBQi1-l88xM5",
        "outputId": "c0cf82ed-484c-4195-b91a-d4afb8b94664",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp==0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 2.8MB/s \n",
            "\u001b[?25hCollecting neuralcoref==4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/24/0ec7845a5b73b637aa691ff4d1b9b48f3a0f3369f4002a59ffd7a7462fdb/neuralcoref-4.0-cp36-cp36m-manylinux1_x86_64.whl (287kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 30.8MB/s \n",
            "\u001b[?25hCollecting spacy==2.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/da/3a1c54694c2d2f40df82f38a19ae14c6eb24a5a1a0dae87205ebea7a84d8/spacy-2.1.3-cp36-cp36m-manylinux1_x86_64.whl (27.7MB)\n",
            "\u001b[K     |████████████████████████████████| 27.7MB 154kB/s \n",
            "\u001b[?25hCollecting spacy-wordnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/f2/4d8070df0f7a7a9eeed74eb7e9ce3cf41349eb5e06b1e088de9eeca630e2/spacy-wordnet-0.0.4.tar.gz (648kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 40.8MB/s \n",
            "\u001b[?25hCollecting torch==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/57/d5cceb0799c06733eefce80c395459f28970ebb9e896846ce96ab579a3f1/torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |████████████████████████████████| 748.9MB 21kB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 37.2MB/s \n",
            "\u001b[?25hCollecting pytorch-pretrained-bert==0.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 45.9MB/s \n",
            "\u001b[?25hCollecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 46.8MB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 37.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (4.41.1)\n",
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
            "Collecting ujson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/84/e039c6ffc6603f2dfe966972d345d4f650a4ffd74b18c852ece645de12ac/ujson-4.0.1-cp36-cp36m-manylinux1_x86_64.whl (179kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 48.3MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/53/c0a4b5dd73c1bf265331e73f0f62705665f0f7eb25eee41bda97b058163f/boto3-1.16.14-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 46.8MB/s \n",
            "\u001b[?25hCollecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 50.2MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.9MB/s \n",
            "\u001b[?25hCollecting numpydoc>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/1d/9e398c53d6ae27d5ab312ddc16a9ffe1bee0dfdf1d6ec88c40b0ca97582e/numpydoc-1.1.0-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.3MB/s \n",
            "\u001b[?25hCollecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/69/7f/d0aeaaafb5c3c76c8d2141dbe2d4f6dca5d6c31872d4e5349768c1958abc/Flask_Cors-3.0.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9->-r requirements.txt (line 1)) (0.5.3)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9->-r requirements.txt (line 1)) (1.4.1)\n",
            "Collecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/c1/04/8a5258cfd851c9c89ae5c12c6952c05d42ca8c0788b999806e0c78d06c54/responses-0.12.0-py2.py3-none-any.whl\n",
            "Collecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9->-r requirements.txt (line 1)) (1.1.2)\n",
            "Collecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.1MB/s \n",
            "\u001b[?25hCollecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/af/ca/4fee219cc4113a5635e348ad951cf8a2e47fed2e3342312493f5b73d0007/jsonpickle-1.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9->-r requirements.txt (line 1)) (0.4.1)\n",
            "Collecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9->-r requirements.txt (line 1)) (0.22.2.post1)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/70/ed1ba808a87d896b9f4d25400dda54e089ca7a97e87cee620b3744997c89/jsonnet-0.16.0.tar.gz (256kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 45.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9->-r requirements.txt (line 1)) (1.18.5)\n",
            "Collecting gevent>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/92/b80b922f08f222faca53c8d278e2e612192bc74b0e1f0db2f80a6ee46982/gevent-20.9.0-cp36-cp36m-manylinux2010_x86_64.whl (5.3MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3MB 31.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9->-r requirements.txt (line 1)) (3.2.2)\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/43/0e/2f50064e327f41a1eb811df089f813036e19a64b95e33f8e9e0b96c2447e/flaky-3.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9->-r requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9->-r requirements.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9->-r requirements.txt (line 1)) (3.6.4)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (1.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (1.0.2)\n",
            "Collecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (0.8.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (2.0.4)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (2.6.0)\n",
            "Collecting thinc<7.1.0,>=7.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 40.4MB/s \n",
            "\u001b[?25hCollecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 35.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5->-r requirements.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.6.2->-r requirements.txt (line 7)) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r requirements.txt (line 9)) (3.12.4)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.20.0,>=1.19.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/6a/466d8644e61192e2b36db2aca2ebca952ad527220f9776b87e94de1a172d/botocore-1.19.14-py2.py3-none-any.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 42.6MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp==0.9->-r requirements.txt (line 1)) (0.2.5)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp==0.9->-r requirements.txt (line 1)) (2.11.2)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp==0.9->-r requirements.txt (line 1)) (1.8.5)\n",
            "Collecting urllib3>=1.25.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp==0.9->-r requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp==0.9->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp==0.9->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp==0.9->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp==0.9->-r requirements.txt (line 1)) (0.17.0)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/b0/da8afd9b3bd50c7665ecdac062f182982af1173c9081f9af7261091c5588/zope.interface-5.2.0-cp36-cp36m-manylinux2010_x86_64.whl (236kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 44.6MB/s \n",
            "\u001b[?25hCollecting greenlet>=0.4.17; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d0/532e160c777b42f6f393f9de8c88abb8af6c892037c55e4d3a8a211324dd/greenlet-0.4.17-cp36-cp36m-manylinux1_x86_64.whl (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp==0.9->-r requirements.txt (line 1)) (50.3.2)\n",
            "Collecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==0.9->-r requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==0.9->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==0.9->-r requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==0.9->-r requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==0.9->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==0.9->-r requirements.txt (line 1)) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==0.9->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9->-r requirements.txt (line 1)) (1.9.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9->-r requirements.txt (line 1)) (8.6.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9->-r requirements.txt (line 1)) (20.2.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9->-r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp==0.9->-r requirements.txt (line 1)) (1.1.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9->-r requirements.txt (line 1)) (0.7.12)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9->-r requirements.txt (line 1)) (2.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9->-r requirements.txt (line 1)) (20.4)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9->-r requirements.txt (line 1)) (0.16)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9->-r requirements.txt (line 1)) (1.2.4)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9->-r requirements.txt (line 1)) (2.8.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle->allennlp==0.9->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.6/dist-packages (from sphinxcontrib-websupport->sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9->-r requirements.txt (line 1)) (1.1.4)\n",
            "Building wheels for collected packages: spacy-wordnet, nltk, ftfy, word2number, parsimonious, overrides, jsonnet\n",
            "  Building wheel for spacy-wordnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy-wordnet: filename=spacy_wordnet-0.0.4-py2.py3-none-any.whl size=650293 sha256=f6e2de852ee5d14e478f5862dbff1c49dbdfe3504b1da23c568c424063e83fe7\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/93/1d/c86db913cd146fc9ddb26d10f56579c5d58a3e00bc8f96a3a6\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449907 sha256=1992719a1fdae82f4235563247c57e536d8f87cd7c7e7ad671d6a7ea4d4e2189\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45612 sha256=cce2d7b55e82c99119f73d6c10180fc4beb9d17de9a0c32386c176d1c35c603b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=574b2013baea6ee9b6081018f8733d69eee26c0ffcc95874d799034e42fa5403\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42709 sha256=95e7f0bb4ac893652a7cc0d0c3365b88e255447e2467e464b8d6b5e13c145d0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp36-none-any.whl size=10174 sha256=aba5c592d0335d52af7628844daf517d547ce7169b835c43372539b31b33fd8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.16.0-cp36-cp36m-linux_x86_64.whl size=3321600 sha256=a6aa7a3003077f1bca4735dccb3c19b8c5e0db732d1e9eb40d159bb228eef640\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/a9/43/bc5e0463deeec89dfca928a2a64595f1bdb520c891f6fbd09c\n",
            "Successfully built spacy-wordnet nltk ftfy word2number parsimonious overrides jsonnet\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy-wordnet 0.0.4 has requirement nltk<3.4,>=3.3, but you'll have nltk 3.4.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, urllib3, botocore, s3transfer, boto3, unidecode, sentencepiece, torch, pytorch-transformers, ftfy, pytorch-pretrained-bert, numpydoc, flask-cors, word2number, nltk, responses, preshed, plac, blis, thinc, spacy, conllu, parsimonious, tensorboardX, jsonpickle, overrides, jsonnet, zope.interface, greenlet, zope.event, gevent, flaky, allennlp, neuralcoref, spacy-wordnet, jsonlines, ujson\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: preshed 3.0.2\n",
            "    Uninstalling preshed-3.0.2:\n",
            "      Successfully uninstalled preshed-3.0.2\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed allennlp-0.9.0 blis-0.2.4 boto3-1.16.14 botocore-1.19.14 conllu-1.3.1 flaky-3.7.0 flask-cors-3.0.9 ftfy-5.8 gevent-20.9.0 greenlet-0.4.17 jmespath-0.10.0 jsonlines-1.2.0 jsonnet-0.16.0 jsonpickle-1.4.1 neuralcoref-4.0 nltk-3.4.5 numpydoc-1.1.0 overrides-3.1.0 parsimonious-0.8.1 plac-0.9.6 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.12.0 s3transfer-0.3.3 sentencepiece-0.1.94 spacy-2.1.3 spacy-wordnet-0.0.4 tensorboardX-2.1 thinc-7.0.8 torch-1.2.0 ujson-4.0.1 unidecode-1.1.1 urllib3-1.25.11 word2number-1.1 zope.event-4.5.0 zope.interface-5.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crjqeCFK-Ny1",
        "outputId": "de0ba899-1aa9-44d9-8f21-cde2d85110e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "\n",
        "%cd /content/gen_bert/injecting_numeracy/textual_data_generation"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gen_bert/injecting_numeracy/textual_data_generation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNmY5eD78ei9",
        "outputId": "ecfcfd2a-4753-43ac-b391-f4404946e001",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.1.4)\n",
            "Requirement already satisfied: spacy==2.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.41.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 2)) (2.8.1)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (0.2.4)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (2.6.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (0.9.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (0.8.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (2.0.4)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (7.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (1.0.3)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (2.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3->-r requirements.txt (line 3)) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3->-r requirements.txt (line 3)) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3->-r requirements.txt (line 3)) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-Kp4bgE_cvO",
        "outputId": "df7f8d7d-a094-4438-b307-e4a006cda285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 3.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-cp36-none-any.whl size=11074435 sha256=79076c575c6ce939b7096d58f881ba5a4c036106a33aafa05b54b4fcd548b640\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xxwg3sar/wheels/39/ea/3b/507f7df78be8631a7a3d7090962194cf55bc1158572c0be77f\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOzKmBDH_hhQ",
        "outputId": "e1d9e65d-6a42-4ae2-8c6e-c57575f082d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!python generate_examples.py"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0% 0/16 [00:00<?, ?it/s]\r100% 16/16 [00:00<00:00, 606.17it/s]\n",
            "number of generated history questions: 486\n",
            "cases of empty history examples: 0\n",
            "number of generated history questions: 0\n",
            "cases of empty history examples: 0\n",
            "\r  0% 0/4 [00:00<?, ?it/s]\r100% 4/4 [00:00<00:00, 655.54it/s]\n",
            "number of generated nfl questions: 119\n",
            "cases of empty nfl examples: 0\n",
            "number of generated nfl questions: 0\n",
            "cases of empty nfl examples: 0\n",
            "total number of generated questions: 605\n",
            "total cases of empty examples: 0\n",
            "total number of evaluation questions: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onVeAU9V_srm",
        "outputId": "d60e1db2-be37-4d1b-913a-111c31504d7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "%cd /content/gen_bert/injecting_numeracy/pre_training"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gen_bert/injecting_numeracy/pre_training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_KZ0k0C_y72"
      },
      "source": [
        "!bash download.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVMwjw34Ao0A",
        "outputId": "30169466-09f6-4c0e-d0a0-b0551248e7b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gen_bert/injecting_numeracy/pre_training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4lBPr6aFNCD"
      },
      "source": [
        "!python gen_train_data_MLM.py --train_corpus ./data/MLM_paras.jsonl --bert_model bert-base-uncased --output_dir ./data/MLM_train/ --do_lower_case --max_predictions_per_seq 65 --digitize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twwEjzVlFqjv",
        "outputId": "79b94ca9-dbed-4eea-897c-254b18c7f915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "%cd /content/gen_bert/injecting_numeracy/pre_training/gen_bert"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gen_bert/injecting_numeracy/pre_training/gen_bert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLYA_NOYl_5I",
        "outputId": "2ab1df34-91f4-4f9a-9d03-96ad43e57d76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "#drop train feature create\n",
        "!python create_examples_n_features.py --split train --drop_json ../data/new_drop_dataset_train.json --output_dir data/examples_n_features --max_seq_length 512"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:__main__:creating examples\n",
            "INFO:__main__:Reading file at ../data/new_drop_dataset_train.json\n",
            "100% 1000/1000 [02:14<00:00,  7.44it/s]\n",
            "INFO:__main__:Skipped 2 examples, kept 13822 examples.\n",
            "INFO:__main__:creating features\n",
            "100% 13822/13822 [01:13<00:00, 188.86it/s]\n",
            "INFO:__main__:Skipped 0 features, truncated 1019 features, kept 13822 features.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeWo-u8QmQm6",
        "outputId": "def33d31-5cba-49a4-b01f-fe83a56a69bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "#drop eval feature create\n",
        "!python create_examples_n_features.py --split eval --drop_json ../data/small_dev.json --output_dir data/examples_n_features --max_seq_length 512"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:__main__:creating examples\n",
            "INFO:__main__:Reading file at ../data/small_dev.json\n",
            "100% 29/29 [00:03<00:00,  8.47it/s]\n",
            "INFO:__main__:Skipped 0 examples, kept 416 examples.\n",
            "INFO:__main__:creating features\n",
            "100% 416/416 [00:02<00:00, 185.81it/s]\n",
            "INFO:__main__:Skipped 0 features, truncated 0 features, kept 416 features.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dZEKUNLnK2W",
        "outputId": "c0818480-ca67-4221-d247-25f0bd0b107f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "#numeric data train\n",
        "!python create_examples_n_features.py --split train --drop_json ../data/synthetic_numeric_train_drop_format.json --output_dir data/examples_n_features_numeric --max_seq_length 50 --max_decoding_steps 11 --max_n_samples -1"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:__main__:creating examples\n",
            "INFO:__main__:Reading file at ../data/synthetic_numeric_train_drop_format.json\n",
            "100% 990000/990000 [27:05<00:00, 609.16it/s]\n",
            "INFO:__main__:Skipped 0 examples, kept 990000 examples.\n",
            "INFO:__main__:creating features\n",
            "100% 990000/990000 [05:38<00:00, 2925.50it/s]\n",
            "INFO:__main__:Skipped 0 features, truncated 5197 features, kept 990000 features.\n",
            "tcmalloc: large alloc 1073741824 bytes == 0x93402000 @  0x7f610ecdd1e7 0x5aca7b 0x4e0513 0x4e221a 0x4e29d0 0x4e0df8 0x4e255b 0x4e22bf 0x4e33c6 0x4544bc 0x50a25a 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x50ad03 0x634e72 0x634f27 0x6386df 0x639281 0x4b0dc0 0x7f610e8dabf7 0x5b259a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uggwbUrvnLAz",
        "outputId": "1a583b4d-8ee7-42b7-f9c3-e96dd976156c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "#numeric data eval\n",
        "!python create_examples_n_features.py --split eval --drop_json ../data/synthetic_numeric_dev_drop_format.json --output_dir data/examples_n_features_numeric --max_seq_length 50 --max_decoding_steps 11"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:__main__:creating examples\n",
            "INFO:__main__:Reading file at ../data/synthetic_numeric_dev_drop_format.json\n",
            "100% 9996/9996 [00:16<00:00, 622.39it/s]\n",
            "INFO:__main__:Skipped 0 examples, kept 9996 examples.\n",
            "INFO:__main__:creating features\n",
            "100% 9996/9996 [00:03<00:00, 2908.04it/s]\n",
            "INFO:__main__:Skipped 0 features, truncated 53 features, kept 9996 features.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy4KxQnFnLJQ"
      },
      "source": [
        "#texual data train\n",
        "!python create_examples_n_features.py --split train --drop_json ../data/synthetic_textual_mixed_min3_max6_up0.7_train_drop_format.json --output_dir data/examples_n_features_syntext --max_seq_length 160 --max_n_samples -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELYLof4knZzo"
      },
      "source": [
        "#texual data eval\n",
        "!python create_examples_n_features.py --split eval --drop_json ../data/synthetic_textual_mixed_min3_max6_up0.7_dev_drop_format.json --output_dir data/examples_n_features_syntext --max_seq_length 160"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2ZKNbtNoENt"
      },
      "source": [
        "# **Pre-Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4FgjqYln_Yv",
        "outputId": "fc9965c7-bcb2-47d0-d6d0-f36a34cfb95f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# GenBERT + ND\n",
        "!CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python finetune_on_drop.py  --do_train   --do_eval  --mlm_dir ../data/MLM_train/ --examples_n_features_dir ./data/examples_n_features_numeric/ --train_batch_size 200 --mlm_batch_size 48 --mlm_scale 0.5 --eval_batch_size 300 --learning_rate 6e-5  --max_seq_length 50 --num_train_epochs 60.0 --warmup_proportion 0.1 --output_dir out_numeric_finetune_bert --random_shift --num_train_samples -1"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/10/2020 01:54:01 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "11/10/2020 01:54:01 - INFO - __main__ -   device: cpu n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "11/10/2020 01:54:02 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "11/10/2020 01:54:02 - INFO - modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.pytorch_pretrained_bert/distributed_-1/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "11/10/2020 01:54:02 - INFO - modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.pytorch_pretrained_bert/distributed_-1/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "11/10/2020 01:54:02 - INFO - modeling -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/10/2020 01:54:14 - INFO - modeling -   Weights of BertTransformer not initialized from pretrained model: ['extra_enc_layer.dense.weight', 'extra_enc_layer.dense.bias', 'extra_enc_layer.LayerNorm.weight', 'extra_enc_layer.LayerNorm.bias', 'embed.weight', 'qa_head._answer_ability_predictor.0.weight', 'qa_head._answer_ability_predictor.0.bias', 'qa_head._answer_ability_predictor.3.weight', 'qa_head._answer_ability_predictor.3.bias', 'qa_head._passage_weights_predictor.weight', 'qa_head._passage_weights_predictor.bias', 'qa_head._question_weights_predictor.weight', 'qa_head._question_weights_predictor.bias', 'qa_head._passage_span_start_predictor.weight', 'qa_head._passage_span_start_predictor.bias', 'qa_head._passage_span_end_predictor.weight', 'qa_head._passage_span_end_predictor.bias', 'qa_head._question_span_start_predictor.0.weight', 'qa_head._question_span_start_predictor.0.bias', 'qa_head._question_span_start_predictor.3.weight', 'qa_head._question_span_start_predictor.3.bias', 'qa_head._question_span_end_predictor.0.weight', 'qa_head._question_span_end_predictor.0.bias', 'qa_head._question_span_end_predictor.3.weight', 'qa_head._question_span_end_predictor.3.bias', 'extra_dec_layer.dense.weight', 'extra_dec_layer.dense.bias', 'extra_dec_layer.LayerNorm.weight', 'extra_dec_layer.LayerNorm.bias', 'head_type.weight', 'head_type.bias', 'dec_head.bias', 'dec_head.decoder.weight']\n",
            "11/10/2020 01:54:14 - INFO - modeling -   Weights from pretrained model not used in BertTransformer: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "11/10/2020 01:54:14 - INFO - root -   Loading eval examples and features.\n",
            "\n",
            "11/10/2020 01:54:15 - INFO - __main__ -   ***** Running evaluation *****\n",
            "11/10/2020 01:54:15 - INFO - __main__ -     Num examples = 9996\n",
            "11/10/2020 01:54:15 - INFO - __main__ -     Batch size = 300\n",
            "11/10/2020 01:54:15 - INFO - root -   Loading train examples and features.\n",
            "\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLsv__y3n_2I"
      },
      "source": [
        "#GenBERT + TD\n",
        "!CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python finetune_on_drop.py   --do_train   --do_eval  --mlm_dir ../data/MLM_train/ --examples_n_features_dir ./data/examples_n_features_syntext/ --train_batch_size 240 --mlm_batch_size 48 --mlm_scale 0.5 --eval_batch_size 1000 --learning_rate 1e-5  --max_seq_length 160 --num_train_epochs 5.0 --warmup_proportion 0.1 --output_dir out_syntext_finetune_bert --random_shift --num_train_samples -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UUYh5j9ojw8"
      },
      "source": [
        "#GenBERT + ND + TD\n",
        "!CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python train_textual_with_numeric.py  --do_train   --do_eval --mlm_dir ../data/MLM_train/ --examples_n_features_dir_syntext ./data/examples_n_features_syntext/ --examples_n_features_dir_numeric ./data/examples_n_features_numeric/ --train_batch_size_syntext 240 --train_batch_size_numeric 624 --mlm_batch_size 48 --mlm_scale 0.5 --eval_batch_size 1000 --learning_rate 1e-5 --num_train_epochs 5.0 --warmup_proportion 0.1 --output_dir out_syntext_and_numeric_finetune_numeric --init_weights_dir out_numeric_finetune_bert --random_shift --num_train_samples -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct1OCnSuoqsw"
      },
      "source": [
        "# **Fine-Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbmKuBonoj2o",
        "outputId": "38201e21-c652-4065-e0d3-cbff042d0655",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "#GenBERT + DROP\n",
        "!CUDA_VISIBLE_DEVICES=0,1,2,3 python finetune_on_drop.py   --do_train   --do_eval  --examples_n_features_dir ./data/examples_n_features/ --train_batch_size 4 --mlm_batch_size -1 --eval_batch_size 4 --learning_rate 3e-5  --max_seq_length 512 --num_train_epochs 1.0 --warmup_proportion 0.1 --output_dir out_drop_finetune_bert --num_train_samples -1"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/10/2020 03:20:00 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "11/10/2020 03:20:01 - INFO - __main__ -   device: cpu n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "11/10/2020 03:20:01 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "11/10/2020 03:20:01 - INFO - modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.pytorch_pretrained_bert/distributed_-1/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "11/10/2020 03:20:01 - INFO - modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.pytorch_pretrained_bert/distributed_-1/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "11/10/2020 03:20:01 - INFO - modeling -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/10/2020 03:20:13 - INFO - modeling -   Weights of BertTransformer not initialized from pretrained model: ['extra_enc_layer.dense.weight', 'extra_enc_layer.dense.bias', 'extra_enc_layer.LayerNorm.weight', 'extra_enc_layer.LayerNorm.bias', 'embed.weight', 'qa_head._answer_ability_predictor.0.weight', 'qa_head._answer_ability_predictor.0.bias', 'qa_head._answer_ability_predictor.3.weight', 'qa_head._answer_ability_predictor.3.bias', 'qa_head._passage_weights_predictor.weight', 'qa_head._passage_weights_predictor.bias', 'qa_head._question_weights_predictor.weight', 'qa_head._question_weights_predictor.bias', 'qa_head._passage_span_start_predictor.weight', 'qa_head._passage_span_start_predictor.bias', 'qa_head._passage_span_end_predictor.weight', 'qa_head._passage_span_end_predictor.bias', 'qa_head._question_span_start_predictor.0.weight', 'qa_head._question_span_start_predictor.0.bias', 'qa_head._question_span_start_predictor.3.weight', 'qa_head._question_span_start_predictor.3.bias', 'qa_head._question_span_end_predictor.0.weight', 'qa_head._question_span_end_predictor.0.bias', 'qa_head._question_span_end_predictor.3.weight', 'qa_head._question_span_end_predictor.3.bias', 'extra_dec_layer.dense.weight', 'extra_dec_layer.dense.bias', 'extra_dec_layer.LayerNorm.weight', 'extra_dec_layer.LayerNorm.bias', 'head_type.weight', 'head_type.bias', 'dec_head.bias', 'dec_head.decoder.weight']\n",
            "11/10/2020 03:20:13 - INFO - modeling -   Weights from pretrained model not used in BertTransformer: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "11/10/2020 03:20:13 - INFO - root -   Loading eval examples and features.\n",
            "\n",
            "11/10/2020 03:20:14 - INFO - __main__ -   ***** Running evaluation *****\n",
            "11/10/2020 03:20:14 - INFO - __main__ -     Num examples = 3349\n",
            "11/10/2020 03:20:14 - INFO - __main__ -     Batch size = 4\n",
            "11/10/2020 03:20:14 - INFO - root -   Loading train examples and features.\n",
            "\n",
            "11/10/2020 03:20:19 - INFO - __main__ -   ***** Running training *****\n",
            "11/10/2020 03:20:19 - INFO - __main__ -     Num examples = 13822\n",
            "11/10/2020 03:20:19 - INFO - __main__ -     Batch size = 4\n",
            "11/10/2020 03:20:19 - INFO - __main__ -     Num steps = 3456\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/3456 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/3456 [00:34<33:18:30, 34.71s/it]\u001b[A\n",
            "Iteration:   0% 2/3456 [01:07<32:47:14, 34.17s/it]\u001b[A\n",
            "Iteration:   0% 3/3456 [01:39<32:14:34, 33.62s/it]\u001b[A\n",
            "Iteration:   0% 4/3456 [02:12<31:52:14, 33.24s/it]\u001b[A\n",
            "Iteration:   0% 5/3456 [02:44<31:36:51, 32.98s/it]\u001b[A\n",
            "Iteration:   0% 6/3456 [03:17<31:28:05, 32.84s/it]\u001b[A\n",
            "Iteration:   0% 7/3456 [03:49<31:25:29, 32.80s/it]\u001b[A\n",
            "Iteration:   0% 8/3456 [04:23<31:32:22, 32.93s/it]\u001b[A\n",
            "Iteration:   0% 9/3456 [04:55<31:22:11, 32.76s/it]\u001b[A\n",
            "Iteration:   0% 10/3456 [05:27<31:14:42, 32.64s/it]\u001b[A\n",
            "Iteration:   0% 11/3456 [06:00<31:05:28, 32.49s/it]\u001b[A\n",
            "Iteration:   0% 12/3456 [06:32<30:56:29, 32.34s/it]\u001b[A\n",
            "Iteration:   0% 13/3456 [08:00<47:09:56, 49.32s/it]\u001b[A\n",
            "Iteration:   0% 14/3456 [09:12<53:28:45, 55.93s/it]\u001b[A\n",
            "Iteration:   0% 15/3456 [09:44<46:42:40, 48.87s/it]\u001b[A\n",
            "Iteration:   0% 16/3456 [10:45<38:34:05, 40.36s/it]\n",
            "Epoch:   0% 0/1 [10:45<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"finetune_on_drop.py\", line 748, in <module>\n",
            "    main()\n",
            "  File \"finetune_on_drop.py\", line 464, in main\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 118, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 93, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAKpwA9Koj-c"
      },
      "source": [
        "#GenBERT + ND + DROP\n",
        "!CUDA_VISIBLE_DEVICES=0,1,2,3 python finetune_on_drop.py   --do_train   --do_eval  --examples_n_features_dir ./data/examples_n_features/ --train_batch_size 16 --mlm_batch_size -1 --eval_batch_size 360 --learning_rate 3e-5  --max_seq_length 512 --num_train_epochs 30.0 --warmup_proportion 0.1 --init_weights_dir out_numeric_finetune_bert --output_dir out_drop_finetune_numeric --num_train_samples -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z69AaHtroz11"
      },
      "source": [
        "#GenBERT + TD + DROP\n",
        "!CUDA_VISIBLE_DEVICES=0,1,2,3 python finetune_on_drop.py   --do_train   --do_eval  --examples_n_features_dir ./data/examples_n_features/ --train_batch_size 14 --mlm_batch_size -1 --eval_batch_size 400 --learning_rate 3e-5  --max_seq_length 512 --num_train_epochs 30.0 --warmup_proportion 0.1 --init_weights_dir out_syntext_finetune_bert --output_dir out_drop_finetune_syntext --num_train_samples -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn9GKhqFoz_w"
      },
      "source": [
        "#GenBERT + ND + TD + DROP\n",
        "!CUDA_VISIBLE_DEVICES=0,1,2,3 python finetune_on_drop.py   --do_train   --do_eval  --examples_n_features_dir ./data/examples_n_features/ --train_batch_size 14 --mlm_batch_size -1 --eval_batch_size 400 --learning_rate 3e-5  --max_seq_length 512 --num_train_epochs 30.0 --warmup_proportion 0.1 --init_weights_dir out_syntext_and_numeric_finetune_numeric --output_dir out_drop_finetune_syntext_and_numeric --num_train_samples -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk1gbRHWpMqn"
      },
      "source": [
        "**Inference**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDw8ldfSpP14",
        "outputId": "3b384164-d4c5-4795-ff58-bd8d9fc32b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1,2,3 python finetune_on_drop.py --do_eval --do_inference --examples_n_features_dir ./data/examples_n_features/ --eval_batch_size 200 --init_weights_dir out_drop_finetune_bert  --output_dir preds"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/10/2020 16:59:19 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "11/10/2020 16:59:19 - INFO - __main__ -   device: cpu n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "11/10/2020 16:59:19 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "11/10/2020 16:59:19 - INFO - modeling -   loading weights file out_drop_finetune_bert/pytorch_model.bin\n",
            "11/10/2020 16:59:19 - INFO - modeling -   loading configuration file out_drop_finetune_bert/config.json\n",
            "11/10/2020 16:59:19 - INFO - modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/10/2020 16:59:24 - INFO - root -   Loading eval examples and features.\n",
            "\n",
            "11/10/2020 16:59:24 - INFO - __main__ -   ***** Running evaluation *****\n",
            "11/10/2020 16:59:24 - INFO - __main__ -     Num examples = 416\n",
            "11/10/2020 16:59:24 - INFO - __main__ -     Batch size = 200\n",
            "Inference:   0% 0/3 [00:00<?, ?it/s]tcmalloc: large alloc 2516582400 bytes == 0xc2b36000 @  0x7f2710c91b6b 0x7f2710cb1379 0x7f26b165aa4a 0x7f26b165c4fa 0x7f26b3aa002b 0x7f26b364b7ae 0x7f26b356a2ee 0x7f26b356dc4a 0x7f26b356dd8f 0x7f26b385f1e8 0x7f26b5262af0 0x7f26b3567938 0x7f26b35686f3 0x7f26b3a08168 0x7f26b51dd648 0x7f26fb7745b6 0x50a4a5 0x50beb4 0x507be4 0x508ec2 0x594a01 0x59fd0e 0x50d256 0x507be4 0x508ec2 0x594a01 0x54a971 0x5a9dac 0x50a433 0x50beb4 0x507be4\n",
            "tcmalloc: large alloc 2516582400 bytes == 0x158b40000 @  0x7f2710c91b6b 0x7f2710cb1379 0x7f26b165aa4a 0x7f26b165c4fa 0x7f26b3aa002b 0x7f26b36a8e9e 0x7f26b385dd3a 0x7f26b36bd5a1 0x7f26b36c18e4 0x7f26b36c1fbc 0x7f26b341c908 0x7f26b3a060d8 0x7f26b548bc40 0x7f26fb6998f2 0x566f73 0x59fd0e 0x4b1eea 0x6196df 0x59c9f0 0x50ea2d 0x507be4 0x508ec2 0x594a01 0x59fd0e 0x50d256 0x507be4 0x508ec2 0x594a01 0x54a971 0x5a9dac 0x50a433\n",
            "tcmalloc: large alloc 2516582400 bytes == 0xc2b36000 @  0x7f2710c91b6b 0x7f2710cb1379 0x7f26b165aa4a 0x7f26b165c4fa 0x7f26b3aa002b 0x7f26b36a8e9e 0x7f26b385dd3a 0x7f26b36bd5a1 0x7f26b36c18e4 0x7f26b36c1fbc 0x7f26b341bd29 0x7f26b385e895 0x7f26b53ab683 0x7f26fb6c1eed 0x566f73 0x59fd0e 0x4b1eea 0x619e0b 0x59edcf 0x50c604 0x507be4 0x508ec2 0x594a01 0x59fd0e 0x50d256 0x507be4 0x508ec2 0x594a01 0x54a971 0x5a9dac 0x50a433\n",
            "tcmalloc: large alloc 2516582400 bytes == 0x158b40000 @  0x7f2710c91b6b 0x7f2710cb1379 0x7f26b165aa4a 0x7f26b165c4fa 0x7f26b36a8803 0x7f26b385e548 0x7f26b36a9093 0x7f26b36aa434 0x7f26b36aae6d 0x7f26b36611d8 0x7f26b3861e1f 0x7f26b5144503 0x7f26b36600ec 0x7f26b3a0adb3 0x7f26b50ccce9 0x7f26fb63f401 0x50a4a5 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50cc96 0x508cd5 0x594a01 0x59fd0e 0x50d256 0x507be4 0x508ec2 0x594a01 0x54a971 0x5a9dac\n",
            "tcmalloc: large alloc 2516582400 bytes == 0x158b40000 @  0x7f2710c91b6b 0x7f2710cb1379 0x7f26b165aa4a 0x7f26b165c4fa 0x7f26b3aa002b 0x7f26b364b7ae 0x7f26b356a2ee 0x7f26b356dc4a 0x7f26b356dd8f 0x7f26b385f1e8 0x7f26b5262af0 0x7f26b3567938 0x7f26b35686f3 0x7f26b3a08168 0x7f26b51dd648 0x7f26fb7745b6 0x50a4a5 0x50beb4 0x507be4 0x508ec2 0x594a01 0x59fd0e 0x50d256 0x507be4 0x508ec2 0x594a01 0x54a971 0x5a9dac 0x50a433 0x50beb4 0x507be4\n",
            "tcmalloc: large alloc 2516582400 bytes == 0xc2b36000 @  0x7f2710c91b6b 0x7f2710cb1379 0x7f26b165aa4a 0x7f26b165c4fa 0x7f26b3aa002b 0x7f26b36a8e9e 0x7f26b385dd3a 0x7f26b36bd5a1 0x7f26b36c18e4 0x7f26b36c1fbc 0x7f26b341c908 0x7f26b3a060d8 0x7f26b548bc40 0x7f26fb6998f2 0x566f73 0x59fd0e 0x4b1eea 0x6196df 0x59c9f0 0x50ea2d 0x507be4 0x508ec2 0x594a01 0x59fd0e 0x50d256 0x507be4 0x508ec2 0x594a01 0x54a971 0x5a9dac 0x50a433\n",
            "tcmalloc: large alloc 2516582400 bytes == 0x158b40000 @  0x7f2710c91b6b 0x7f2710cb1379 0x7f26b165aa4a 0x7f26b165c4fa 0x7f26b3aa002b 0x7f26b36a8e9e 0x7f26b385dd3a 0x7f26b36bd5a1 0x7f26b36c18e4 0x7f26b36c1fbc 0x7f26b341bd29 0x7f26b385e895 0x7f26b53ab683 0x7f26fb6c1eed 0x566f73 0x59fd0e 0x4b1eea 0x619e0b 0x59edcf 0x50c604 0x507be4 0x508ec2 0x594a01 0x59fd0e 0x50d256 0x507be4 0x508ec2 0x594a01 0x54a971 0x5a9dac 0x50a433\n",
            "tcmalloc: large alloc 2516582400 bytes == 0xc2b36000 @  0x7f2710c91b6b 0x7f2710cb1379 0x7f26b165aa4a 0x7f26b165c4fa 0x7f26b36a8803 0x7f26b385e548 0x7f26b36a9093 0x7f26b36aa434 0x7f26b36aae6d 0x7f26b36611d8 0x7f26b3861e1f 0x7f26b5144503 0x7f26b36600ec 0x7f26b3a0adb3 0x7f26b50ccce9 0x7f26fb63f401 0x50a4a5 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50cc96 0x508cd5 0x594a01 0x59fd0e 0x50d256 0x507be4 0x508ec2 0x594a01 0x54a971 0x5a9dac\n",
            "Inference: 100% 3/3 [54:40<00:00, 1093.36s/it]\n",
            "11/10/2020 17:54:04 - INFO - __main__ -   EM: 0.3004807692307692, Drop EM: 0.3629807692307692\n",
            "11/10/2020 17:54:04 - INFO - __main__ -   saving predictions.jsonl in preds\n",
            "6 7 || 6 1 || 3 5 || 6 3 || 3 3 || 3 6 || Mohnyin nyaungshwe; mone || 3 3 || 3 3 || 17 2 || 3 2 || 2 1 || 4 2 || april 14 1606 april 1605 || Shan state of Mone shan state of mone || 2 2 || april 14 1606 april 1605 || sultan Mehmed V sultan mehmed v || 6 1 || Albanian southern leaders albanian southern leaders || "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1u9gy1TZDSE"
      },
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1,2,3 python finetune_on_drop.py --do_eval --do_inference --examples_n_features_dir ./data/examples_n_features/ --eval_batch_size 200 --init_weights_dir out_drop_finetune_numeric  --output_dir preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpKzIY0QZFOR",
        "outputId": "8388020c-50dd-4db9-ca9f-9109b9734b96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1,2,3 python finetune_on_drop.py --do_eval --do_inference --examples_n_features_dir ./data/examples_n_features/ --eval_batch_size 200 --init_weights_dir out_drop_finetune_syntext_and_numeric  --output_dir preds"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/10/2020 18:25:34 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "11/10/2020 18:25:34 - INFO - __main__ -   device: cpu n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "11/10/2020 18:25:35 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "11/10/2020 18:25:35 - INFO - modeling -   loading weights file out_drop_finetune_syntext_and_numeric/pytorch_model.bin\n",
            "11/10/2020 18:25:35 - INFO - modeling -   loading configuration file out_drop_finetune_syntext_and_numeric/config.json\n",
            "11/10/2020 18:25:35 - INFO - modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/10/2020 18:25:47 - INFO - root -   Loading eval examples and features.\n",
            "\n",
            "11/10/2020 18:25:47 - INFO - __main__ -   ***** Running evaluation *****\n",
            "11/10/2020 18:25:47 - INFO - __main__ -     Num examples = 416\n",
            "11/10/2020 18:25:47 - INFO - __main__ -     Batch size = 200\n",
            "Inference:   0% 0/3 [00:00<?, ?it/s]tcmalloc: large alloc 2516582400 bytes == 0xc21d8000 @  0x7f5422b4ab6b 0x7f5422b6a379 0x7f53c3513a4a 0x7f53c35154fa 0x7f53c595902b 0x7f53c55047ae 0x7f53c54232ee 0x7f53c5426c4a 0x7f53c5426d8f 0x7f53c57181e8 0x7f53c711baf0 0x7f53c5420938 0x7f53c54216f3 0x7f53c58c1168 0x7f53c7096648 0x7f540d62d5b6 0x50a4a5 0x50beb4 0x507be4 0x508ec2 0x594a01 0x59fd0e 0x50d256 0x507be4 0x508ec2 0x594a01 0x54a971 0x5a9dac 0x50a433 0x50beb4 0x507be4\n",
            "tcmalloc: large alloc 2516582400 bytes == 0x1581e2000 @  0x7f5422b4ab6b 0x7f5422b6a379 0x7f53c3513a4a 0x7f53c35154fa 0x7f53c595902b 0x7f53c5561e9e 0x7f53c5716d3a 0x7f53c55765a1 0x7f53c557a8e4 0x7f53c557afbc 0x7f53c52d5908 0x7f53c58bf0d8 0x7f53c7344c40 0x7f540d5528f2 0x566f73 0x59fd0e 0x4b1eea 0x6196df 0x59c9f0 0x50ea2d 0x507be4 0x508ec2 0x594a01 0x59fd0e 0x50d256 0x507be4 0x508ec2 0x594a01 0x54a971 0x5a9dac 0x50a433\n",
            "tcmalloc: large alloc 2516582400 bytes == 0xc21d8000 @  0x7f5422b4ab6b 0x7f5422b6a379 0x7f53c3513a4a 0x7f53c35154fa 0x7f53c595902b 0x7f53c5561e9e 0x7f53c5716d3a 0x7f53c55765a1 0x7f53c557a8e4 0x7f53c557afbc 0x7f53c52d4d29 0x7f53c5717895 0x7f53c7264683 0x7f540d57aeed 0x566f73 0x59fd0e 0x4b1eea 0x619e0b 0x59edcf 0x50c604 0x507be4 0x508ec2 0x594a01 0x59fd0e 0x50d256 0x507be4 0x508ec2 0x594a01 0x54a971 0x5a9dac 0x50a433\n",
            "tcmalloc: large alloc 2516582400 bytes == 0x1581e2000 @  0x7f5422b4ab6b 0x7f5422b6a379 0x7f53c3513a4a 0x7f53c35154fa 0x7f53c5561803 0x7f53c5717548 0x7f53c5562093 0x7f53c5563434 0x7f53c5563e6d 0x7f53c551a1d8 0x7f53c571ae1f 0x7f53c6ffd503 0x7f53c55190ec 0x7f53c58c3db3 0x7f53c6f85ce9 0x7f540d4f8401 0x50a4a5 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50cc96 0x508cd5 0x594a01 0x59fd0e 0x50d256 0x507be4 0x508ec2 0x594a01 0x54a971 0x5a9dac\n",
            "tcmalloc: large alloc 2516582400 bytes == 0x1581e2000 @  0x7f5422b4ab6b 0x7f5422b6a379 0x7f53c3513a4a 0x7f53c35154fa 0x7f53c595902b 0x7f53c55047ae 0x7f53c54232ee 0x7f53c5426c4a 0x7f53c5426d8f 0x7f53c57181e8 0x7f53c711baf0 0x7f53c5420938 0x7f53c54216f3 0x7f53c58c1168 0x7f53c7096648 0x7f540d62d5b6 0x50a4a5 0x50beb4 0x507be4 0x508ec2 0x594a01 0x59fd0e 0x50d256 0x507be4 0x508ec2 0x594a01 0x54a971 0x5a9dac 0x50a433 0x50beb4 0x507be4\n",
            "tcmalloc: large alloc 2516582400 bytes == 0xc21d8000 @  0x7f5422b4ab6b 0x7f5422b6a379 0x7f53c3513a4a 0x7f53c35154fa 0x7f53c595902b 0x7f53c5561e9e 0x7f53c5716d3a 0x7f53c55765a1 0x7f53c557a8e4 0x7f53c557afbc 0x7f53c52d5908 0x7f53c58bf0d8 0x7f53c7344c40 0x7f540d5528f2 0x566f73 0x59fd0e 0x4b1eea 0x6196df 0x59c9f0 0x50ea2d 0x507be4 0x508ec2 0x594a01 0x59fd0e 0x50d256 0x507be4 0x508ec2 0x594a01 0x54a971 0x5a9dac 0x50a433\n",
            "tcmalloc: large alloc 2516582400 bytes == 0x1581e2000 @  0x7f5422b4ab6b 0x7f5422b6a379 0x7f53c3513a4a 0x7f53c35154fa 0x7f53c595902b 0x7f53c5561e9e 0x7f53c5716d3a 0x7f53c55765a1 0x7f53c557a8e4 0x7f53c557afbc 0x7f53c52d4d29 0x7f53c5717895 0x7f53c7264683 0x7f540d57aeed 0x566f73 0x59fd0e 0x4b1eea 0x619e0b 0x59edcf 0x50c604 0x507be4 0x508ec2 0x594a01 0x59fd0e 0x50d256 0x507be4 0x508ec2 0x594a01 0x54a971 0x5a9dac 0x50a433\n",
            "tcmalloc: large alloc 2516582400 bytes == 0xc21d8000 @  0x7f5422b4ab6b 0x7f5422b6a379 0x7f53c3513a4a 0x7f53c35154fa 0x7f53c5561803 0x7f53c5717548 0x7f53c5562093 0x7f53c5563434 0x7f53c5563e6d 0x7f53c551a1d8 0x7f53c571ae1f 0x7f53c6ffd503 0x7f53c55190ec 0x7f53c58c3db3 0x7f53c6f85ce9 0x7f540d4f8401 0x50a4a5 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50cc96 0x508cd5 0x594a01 0x59fd0e 0x50d256 0x507be4 0x508ec2 0x594a01 0x54a971 0x5a9dac\n",
            "Inference: 100% 3/3 [54:09<00:00, 1083.28s/it]\n",
            "11/10/2020 19:19:57 - INFO - __main__ -   EM: 0.5360576923076923, Drop EM: 0.6081730769230769\n",
            "11/10/2020 19:19:57 - INFO - __main__ -   saving predictions.jsonl in preds\n",
            "6 7 || 1 1 || 5 5 || 3 3 || 3 3 || 6 6 || Shan state of Mone nyaungshwe; mone || 4 3 || 3 3 || 3 2 || 1 2 || 1 1 || 2 2 || April april 1605 || Shan state of Mone shan state of mone || 2 2 || April april 1605 || sultan Mehmed V sultan mehmed v || 2 1 || Albanian southern leaders albanian southern leaders || "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG-Z8pPjpZ-t"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TZ-_EZ5pTFG",
        "outputId": "d5874fcd-325e-451d-9933-8743f85d3139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "#simple bert+drop\n",
        "!python drop_eval.py --gold_path ../data/temp_dev.json --prediction_path ./preds/predictions.jsonl --answer_key prediction"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exact-match accuracy 41.18\n",
            "F1 score 43.12\n",
            "41.18   &   43.12\n",
            "----\n",
            "date: 1 (5.88%)\n",
            "  Exact-match accuracy 0.000\n",
            "  F1 score 33.000\n",
            "number: 14 (82.35%)\n",
            "  Exact-match accuracy 42.857\n",
            "  F1 score 42.857\n",
            "span: 2 (11.76%)\n",
            "  Exact-match accuracy 50.000\n",
            "  F1 score 50.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9ZtDpwQoN8-",
        "outputId": "eb567ebe-9af3-420c-bf1b-ba93f02d3fc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "#fintuned with bert + ND + TD\n",
        "!python drop_eval.py --gold_path ../data/temp_dev.json --prediction_path ./preds/predictions.jsonl --answer_key prediction"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exact-match accuracy 64.71\n",
            "F1 score 66.65\n",
            "64.71   &   66.65\n",
            "----\n",
            "date: 1 (5.88%)\n",
            "  Exact-match accuracy 0.000\n",
            "  F1 score 0.000\n",
            "number: 14 (82.35%)\n",
            "  Exact-match accuracy 71.429\n",
            "  F1 score 71.429\n",
            "span: 2 (11.76%)\n",
            "  Exact-match accuracy 50.000\n",
            "  F1 score 66.500\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}