{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Tree.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_-rMNW939V_"
      },
      "source": [
        "**Loading the google drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6TnEbfpp7E9",
        "outputId": "9601d160-f7c5-49f3-c6ac-f2b351452c19"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXFHjvhy4Mwg"
      },
      "source": [
        "**Setting the directory path**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0lGhjAPq2EI",
        "outputId": "1d4ab630-21b7-496f-ac39-73e08a6b5131"
      },
      "source": [
        "%cd /content/gdrive/My\\ Drive/math_seq2tree/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/math_seq2tree\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRZZWsIx4Td8"
      },
      "source": [
        "**Installing the requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_plsIZ6q7kM",
        "outputId": "8dbd0f67-74af-4276-df60-e5cd374bf9ab"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MUG645q4YtO"
      },
      "source": [
        "**Changing the PyTorch Version**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2Y_A8EdtQ02",
        "outputId": "41830ec1-58bc-48c6-fbe0-212c781c831e"
      },
      "source": [
        "!pip uninstall torch\n",
        "!pip install https://download.pytorch.org/whl/cu100/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling torch-1.7.0+cu101:\n",
            "  Would remove:\n",
            "    /usr/local/bin/convert-caffe2-to-onnx\n",
            "    /usr/local/bin/convert-onnx-to-caffe2\n",
            "    /usr/local/lib/python3.6/dist-packages/caffe2/*\n",
            "    /usr/local/lib/python3.6/dist-packages/torch-1.7.0+cu101.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled torch-1.7.0+cu101\n",
            "Collecting torch==1.0.1.post2\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu100/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl (636.8MB)\n",
            "\u001b[K     |████████████████████████████████| 636.8MB 28kB/s \n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.0.1.post2 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-1.0.1.post2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz0Em5j-15aP",
        "outputId": "e5e45cfb-255e-412f-eb4b-27bad099acc4"
      },
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 5.8MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/7f/4ade91fbb684c6f28a6e56028d9f9d2de4297761850d083579779f07c0de/boto3-1.16.25-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 18.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.0.1.post2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.20.0,>=1.19.25\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/d5/c0c33ca15e31062220ac5964f3492409eaf90a5cf5399503cd8264f2f8e9/botocore-1.19.25-py2.py3-none-any.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 13.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.25->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.25->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.19.25 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.16.25 botocore-1.19.25 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCMzLbYYtjuK"
      },
      "source": [
        "**Original Model with embedding size 128**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r46LSnFiszdk",
        "outputId": "dcd8efaa-c525-4f51-f800-f12a2bf47950"
      },
      "source": [
        "!python3 run_seq2tree.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Transfer numbers...\n",
            "(['镇海', '雅乐', '学校', '二年级', '的', '小朋友', '到', '一条', '小路', '的', '一边', '植树', '．', '小朋友', '们', '每隔', 'NUM', '米', '种', '一棵树', '（', '马路', '两头', '都', '种', '了', '树', '）', '，', '最后', '发现', '一共', '种', '了', 'NUM', '棵', '，', '这', '条', '小路', '长', '多少', '米', '．'], ['(', 'N1', '-', '1', ')', '*', 'N0'], ['2', '11'], [16, 34])\n",
            "Indexing words...\n",
            "keep_words 3671 / 9971 = 0.3682\n",
            "Vocab Saved\n",
            "Indexed 3674 words in input language, 23 words in output\n",
            "Number of training data 18530\n",
            "Number of testind data 4632\n",
            "([3, 4, 5, 1, 6, 7, 8, 9, 10, 11, 12, 13, 1, 14, 10, 15, 1, 16, 17, 2, 10, 18, 19, 20, 21, 16, 22], 27, [0, 9, 8], 3, ['5', '0.8', '150'], [3, 12, 16], [])\n",
            "fold: 1\n",
            "epoch: 1\n",
            "loss: 1.3497868589286146\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "1119 1265 4632\n",
            "test_answer_acc 0.241580310880829 0.27310017271157166\n",
            "testing time 0h 5m 28s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 2\n",
            "loss: 0.894130551815033\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 3\n",
            "loss: 0.7327627796551277\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 4\n",
            "loss: 0.6430243143747593\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 5\n",
            "loss: 0.5873636589995745\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 6\n",
            "loss: 0.5423240409842853\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 7\n",
            "loss: 0.5072088540628039\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 8\n",
            "loss: 0.48288807498997655\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 9\n",
            "loss: 0.4631642236791808\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 10\n",
            "loss: 0.4393166654068848\n",
            "training time 0h 2m 33s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 11\n",
            "loss: 0.42312040015541275\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "2657 3004 4632\n",
            "test_answer_acc 0.5736183074265976 0.6485319516407599\n",
            "testing time 0h 5m 1s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 12\n",
            "loss: 0.4094196566733821\n",
            "training time 0h 2m 32s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 13\n",
            "Traceback (most recent call last):\n",
            "  File \"run_seq2tree.py\", line 96, in <module>\n",
            "    encoder_optimizer, predict_optimizer, generate_optimizer, merge_optimizer, output_lang, num_pos_batches[idx])\n",
            "  File \"/content/gdrive/MyDrive/math_seq2tree/src/train_and_evaluate.py\", line 756, in train_tree\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 102, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 90, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXqI6J5n4l0t"
      },
      "source": [
        "**Original Model with embedding size 768**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NZSok73DzU3",
        "outputId": "05b9d299-d37e-4a15-f60f-89e21d08ff43"
      },
      "source": [
        "!python3 run_seq2tree.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Transfer numbers...\n",
            "(['镇海', '雅乐', '学校', '二年级', '的', '小朋友', '到', '一条', '小路', '的', '一边', '植树', '．', '小朋友', '们', '每隔', 'NUM', '米', '种', '一棵树', '（', '马路', '两头', '都', '种', '了', '树', '）', '，', '最后', '发现', '一共', '种', '了', 'NUM', '棵', '，', '这', '条', '小路', '长', '多少', '米', '．'], ['(', 'N1', '-', '1', ')', '*', 'N0'], ['2', '11'], [16, 34])\n",
            "Indexing words...\n",
            "keep_words 3671 / 9971 = 0.3682\n",
            "Indexed 3674 words in input language, 23 words in output\n",
            "Number of training data 18530\n",
            "Number of testind data 4632\n",
            "Vocabs Saved\n",
            "TIL: 3674\n",
            "([3, 4, 5, 1, 6, 7, 8, 9, 10, 11, 12, 13, 1, 14, 10, 15, 1, 16, 17, 2, 10, 18, 19, 20, 21, 16, 22], 27, [0, 9, 8], 3, ['5', '0.8', '150'], [3, 12, 16], [])\n",
            "fold: 1\n",
            "epoch: 1\n",
            "loss: 1.5688643767915924\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "915 1021 4632\n",
            "test_answer_acc 0.19753886010362695 0.22042314335060448\n",
            "testing time 0h 5m 48s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 2\n",
            "loss: 1.1292165809664232\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 3\n",
            "loss: 1.1061704576015472\n",
            "training time 0h 2m 46s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 4\n",
            "loss: 1.188500380310519\n",
            "training time 0h 2m 47s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 5\n",
            "loss: 1.2953213021673005\n",
            "training time 0h 2m 47s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 6\n",
            "loss: 1.348259370902489\n",
            "training time 0h 2m 47s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 7\n",
            "loss: 1.3494106448929886\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 8\n",
            "loss: 1.340556751037466\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 9\n",
            "loss: 1.2997178677854866\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 10\n",
            "loss: 1.3187750281958746\n",
            "training time 0h 2m 50s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 11\n",
            "loss: 1.3001431091078397\n",
            "training time 0h 2m 49s\n",
            "--------------------------------\n",
            "780 869 4632\n",
            "test_answer_acc 0.16839378238341968 0.18760794473229706\n",
            "testing time 0h 5m 17s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 12\n",
            "loss: 1.287707653538934\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 13\n",
            "loss: 1.278057117708798\n",
            "training time 0h 2m 49s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 14\n",
            "loss: 1.2597686751135464\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 15\n",
            "loss: 1.2490163363259414\n",
            "training time 0h 2m 49s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 16\n",
            "loss: 1.268637085372004\n",
            "training time 0h 2m 49s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 17\n",
            "loss: 1.2456469169978437\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 18\n",
            "loss: 1.245704847574234\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 19\n",
            "loss: 1.2338858162534647\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 20\n",
            "loss: 1.2352872453886887\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 21\n",
            "loss: 1.1629196590390698\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "1018 1147 4632\n",
            "test_answer_acc 0.21977547495682212 0.2476252158894646\n",
            "testing time 0h 5m 40s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 22\n",
            "loss: 1.1342953459969882\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 23\n",
            "loss: 1.1229477709737317\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 24\n",
            "loss: 1.1066390997376936\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 25\n",
            "loss: 1.0943298430278383\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 26\n",
            "loss: 1.080027651581271\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 27\n",
            "loss: 1.0730247645542539\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 28\n",
            "loss: 1.0646169717969565\n",
            "training time 0h 2m 51s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 29\n",
            "loss: 1.056603643606449\n",
            "training time 0h 2m 52s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 30\n",
            "loss: 1.0481751746144787\n",
            "training time 0h 2m 52s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 31\n",
            "loss: 1.0395515859127045\n",
            "training time 0h 2m 51s\n",
            "--------------------------------\n",
            "1222 1359 4632\n",
            "test_answer_acc 0.26381692573402415 0.2933937823834197\n",
            "testing time 0h 5m 36s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 32\n",
            "loss: 1.031641158975404\n",
            "training time 0h 2m 49s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 33\n",
            "loss: 1.0269390046596527\n",
            "training time 0h 2m 50s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 34\n",
            "loss: 1.0197416634395204\n",
            "training time 0h 2m 52s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 35\n",
            "loss: 1.010275280886683\n",
            "training time 0h 2m 51s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 36\n",
            "loss: 1.0051723537773922\n",
            "training time 0h 2m 51s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 37\n",
            "loss: 1.0020883356702739\n",
            "training time 0h 2m 51s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 38\n",
            "loss: 0.9993190274156373\n",
            "training time 0h 2m 50s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 39\n",
            "loss: 0.9855311130655223\n",
            "training time 0h 2m 49s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 40\n",
            "loss: 0.9827741670197454\n",
            "training time 0h 2m 49s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 41\n",
            "loss: 0.9438088439661881\n",
            "training time 0h 2m 49s\n",
            "--------------------------------\n",
            "1384 1554 4632\n",
            "test_answer_acc 0.2987910189982729 0.33549222797927464\n",
            "testing time 0h 5m 31s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 42\n",
            "loss: 0.9229881948438184\n",
            "training time 0h 2m 52s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 43\n",
            "loss: 0.9090983257211488\n",
            "training time 0h 2m 52s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 44\n",
            "loss: 0.9036036756531945\n",
            "training time 0h 2m 52s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 45\n",
            "loss: 0.8942589959193921\n",
            "training time 0h 2m 51s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 46\n",
            "loss: 0.8843563708765754\n",
            "training time 0h 2m 51s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 47\n",
            "loss: 0.8777866118940814\n",
            "training time 0h 2m 49s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 48\n",
            "loss: 0.8687731475665652\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 49\n",
            "loss: 0.8651258577560557\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 50\n",
            "loss: 0.8561696077215261\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 51\n",
            "loss: 0.8527734259079243\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "1596 1809 4632\n",
            "test_answer_acc 0.344559585492228 0.3905440414507772\n",
            "testing time 0h 5m 53s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 52\n",
            "loss: 0.848341702592784\n",
            "training time 0h 2m 50s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 53\n",
            "loss: 0.8403413546496424\n",
            "training time 0h 2m 49s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 54\n",
            "loss: 0.8334473120755163\n",
            "training time 0h 2m 50s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 55\n",
            "loss: 0.8325313627719879\n",
            "training time 0h 2m 47s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 56\n",
            "loss: 0.822309927077129\n",
            "training time 0h 2m 47s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 57\n",
            "loss: 0.8137606869483817\n",
            "training time 0h 2m 47s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 58\n",
            "loss: 0.8114735482067897\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 59\n",
            "loss: 0.8080576845284166\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 60\n",
            "loss: 0.8007316410541534\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 61\n",
            "loss: 0.779424642283341\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "1776 2023 4632\n",
            "test_answer_acc 0.38341968911917096 0.43674438687392053\n",
            "testing time 0h 5m 25s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 62\n",
            "loss: 0.7672836274936281\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 63\n",
            "loss: 0.7594638869680207\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 64\n",
            "loss: 0.7486954606812576\n",
            "training time 0h 2m 33s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 65\n",
            "loss: 0.7436270002661081\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 66\n",
            "loss: 0.7412231920094325\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 67\n",
            "loss: 0.7366238380300587\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 68\n",
            "loss: 0.735026738561433\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 69\n",
            "loss: 0.7232265992411252\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 70\n",
            "loss: 0.7213923345352041\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 71\n",
            "loss: 0.7134473595125922\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "1948 2221 4632\n",
            "test_answer_acc 0.42055267702936094 0.4794905008635579\n",
            "testing time 0h 5m 23s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 72\n",
            "loss: 0.7076314595238916\n",
            "training time 0h 2m 33s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 73\n",
            "loss: 0.7121569950005103\n",
            "training time 0h 2m 33s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 74\n",
            "loss: 0.7026630631808577\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 75\n",
            "loss: 0.7001113400377076\n",
            "training time 0h 2m 33s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 76\n",
            "loss: 0.695282636017635\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 77\n",
            "loss: 0.6917977400894822\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "1956 2244 4632\n",
            "test_answer_acc 0.422279792746114 0.4844559585492228\n",
            "testing time 0h 5m 29s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 78\n",
            "loss: 0.6849687730443889\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "1941 2229 4632\n",
            "test_answer_acc 0.41904145077720206 0.48121761658031087\n",
            "testing time 0h 5m 18s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 79\n",
            "loss: 0.6805863783277314\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "2001 2296 4632\n",
            "test_answer_acc 0.4319948186528497 0.49568221070811747\n",
            "testing time 0h 5m 36s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 80\n",
            "loss: 0.6816386422206615\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "1975 2260 4632\n",
            "test_answer_acc 0.4263816925734024 0.48791018998272884\n",
            "testing time 0h 5m 39s\n",
            "------------------------------------------------------\n",
            "Indexing words...\n",
            "keep_words 3687 / 10014 = 0.3682\n",
            "Indexed 3690 words in input language, 23 words in output\n",
            "Number of training data 18530\n",
            "Number of testind data 4632\n",
            "Vocabs Saved\n",
            "TIL: 3690\n",
            "([2, 2, 3, 4, 5, 6, 7, 8, 9, 5, 10, 11, 12, 6, 13, 14, 1, 15, 16, 17, 18, 19, 20, 21, 16, 22, 23, 24, 25, 26, 27, 28, 16, 22, 1, 29, 25, 30, 31, 9, 32, 33, 15, 12], 44, [0, 1, 8, 5, 7], 5, ['2', '11'], [16, 34], [])\n",
            "fold: 2\n",
            "epoch: 1\n",
            "loss: 1.5486828413502924\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "1027 1119 4632\n",
            "test_answer_acc 0.22171848013816925 0.241580310880829\n",
            "testing time 0h 4m 56s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 2\n",
            "loss: 1.135920262336731\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 3\n",
            "loss: 1.127059867464263\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 4\n",
            "loss: 1.1751381559618588\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 5\n",
            "loss: 1.3264169240819996\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 6\n",
            "loss: 1.356503057068792\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 7\n",
            "loss: 1.3259698025111495\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 8\n",
            "loss: 1.2916969722714917\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 9\n",
            "loss: 1.2796404221962239\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 10\n",
            "loss: 1.2553364527636561\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 11\n",
            "loss: 1.242413998883346\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "866 984 4632\n",
            "test_answer_acc 0.18696027633851467 0.21243523316062177\n",
            "testing time 0h 4m 57s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 12\n",
            "loss: 1.2565922835777545\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 13\n",
            "loss: 1.248424601965937\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 14\n",
            "loss: 1.2327106403893437\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 15\n",
            "loss: 1.2510305564979027\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 16\n",
            "loss: 1.2524219961001954\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 17\n",
            "loss: 1.2393052043585941\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 18\n",
            "loss: 1.2522418733300833\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 19\n",
            "loss: 1.2441615343093873\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 20\n",
            "loss: 1.2394468578799018\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 21\n",
            "loss: 1.1732938355412976\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "996 1114 4632\n",
            "test_answer_acc 0.21502590673575128 0.24050086355785838\n",
            "testing time 0h 5m 1s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 22\n",
            "loss: 1.1354673032102913\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 23\n",
            "loss: 1.120412028657979\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 24\n",
            "loss: 1.1023706304615941\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 25\n",
            "loss: 1.0946532498145927\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 26\n",
            "loss: 1.084610374837086\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 27\n",
            "loss: 1.0752648707093864\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 28\n",
            "loss: 1.065298671146919\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 29\n",
            "loss: 1.0563691022067234\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 30\n",
            "loss: 1.0481308651381525\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 31\n",
            "loss: 1.0404348613886998\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "1166 1302 4632\n",
            "test_answer_acc 0.25172711571675305 0.2810880829015544\n",
            "testing time 0h 5m 8s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 32\n",
            "loss: 1.0373798234709377\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 33\n",
            "loss: 1.0249315068639557\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 34\n",
            "loss: 1.0229079406836936\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 35\n",
            "loss: 1.018608624359657\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 36\n",
            "loss: 1.0103636982112096\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 37\n",
            "loss: 1.0056859267169032\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 38\n",
            "loss: 0.9974828800250743\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 39\n",
            "loss: 0.9903809245290427\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 40\n",
            "loss: 0.991266803289282\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 41\n",
            "loss: 0.9534714665906182\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "1406 1592 4632\n",
            "test_answer_acc 0.3035405872193437 0.3436960276338515\n",
            "testing time 0h 5m 11s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 42\n",
            "loss: 0.9313795019840372\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 43\n",
            "loss: 0.9192138285472475\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 44\n",
            "loss: 0.912339573276454\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 45\n",
            "loss: 0.9000700323746122\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 46\n",
            "loss: 0.8932210492676702\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 47\n",
            "loss: 0.8889886036001403\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 48\n",
            "loss: 0.8814240496734093\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 49\n",
            "loss: 0.8735229923807342\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 50\n",
            "loss: 0.8671444816835996\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 51\n",
            "loss: 0.8614570432695849\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "1572 1774 4632\n",
            "test_answer_acc 0.3393782383419689 0.38298791018998274\n",
            "testing time 0h 5m 14s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 52\n",
            "loss: 0.8579595208168029\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 53\n",
            "loss: 0.8492904311624067\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 54\n",
            "loss: 0.8448787816639605\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 55\n",
            "loss: 0.8401267306558017\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 56\n",
            "loss: 0.8355006968152934\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 57\n",
            "loss: 0.8332280313146525\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 58\n",
            "loss: 0.8226052878231838\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 59\n",
            "loss: 0.8205863691609482\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 60\n",
            "loss: 0.8115840866647918\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 61\n",
            "loss: 0.7943397709007921\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "1792 2007 4632\n",
            "test_answer_acc 0.38687392055267705 0.4332901554404145\n",
            "testing time 0h 5m 20s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 62\n",
            "loss: 0.778153932094574\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 63\n",
            "loss: 0.77631061775931\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 64\n",
            "loss: 0.7654837371974156\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 65\n",
            "loss: 0.7603665134002422\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 66\n",
            "loss: 0.7508126137585476\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 67\n",
            "loss: 0.7501931358000328\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 68\n",
            "loss: 0.7428660205725965\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 69\n",
            "loss: 0.7396742806352418\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 70\n",
            "loss: 0.7348720492987797\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 71\n",
            "loss: 0.7327648450588358\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "1846 2091 4632\n",
            "test_answer_acc 0.39853195164075994 0.45142487046632124\n",
            "testing time 0h 5m 19s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 72\n",
            "loss: 0.7269070314949957\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 73\n",
            "loss: 0.7232027148378306\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 74\n",
            "loss: 0.7220425455734647\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 75\n",
            "loss: 0.7160537173008097\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 76\n",
            "loss: 0.7115130989715971\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 77\n",
            "loss: 0.7038551977996168\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "1941 2192 4632\n",
            "test_answer_acc 0.41904145077720206 0.47322970639032813\n",
            "testing time 0h 5m 26s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 78\n",
            "loss: 0.7020628470799019\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "1969 2231 4632\n",
            "test_answer_acc 0.42508635578583764 0.48164939550949915\n",
            "testing time 0h 5m 25s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 79\n",
            "loss: 0.70040689809569\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "1966 2229 4632\n",
            "test_answer_acc 0.42443868739205526 0.48121761658031087\n",
            "testing time 0h 5m 31s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 80\n",
            "loss: 0.6980496295567217\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "1927 2180 4632\n",
            "test_answer_acc 0.4160189982728843 0.47063903281519864\n",
            "testing time 0h 5m 36s\n",
            "------------------------------------------------------\n",
            "Indexing words...\n",
            "keep_words 3675 / 10000 = 0.3675\n",
            "Indexed 3678 words in input language, 23 words in output\n",
            "Number of training data 18530\n",
            "Number of testind data 4632\n",
            "Vocabs Saved\n",
            "TIL: 3678\n",
            "([2, 2, 3, 4, 5, 6, 7, 8, 9, 5, 10, 11, 12, 6, 13, 14, 1, 15, 16, 17, 18, 19, 20, 21, 16, 22, 23, 24, 25, 26, 27, 28, 16, 22, 1, 29, 25, 30, 31, 9, 32, 33, 15, 12], 44, [0, 1, 8, 5, 7], 5, ['2', '11'], [16, 34], [])\n",
            "fold: 3\n",
            "epoch: 1\n",
            "loss: 1.56501040129826\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "735 802 4632\n",
            "test_answer_acc 0.15867875647668395 0.17314335060449051\n",
            "testing time 0h 4m 12s\n",
            "------------------------------------------------------\n",
            "fold: 3\n",
            "epoch: 2\n",
            "loss: 1.1812262783790457\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 3\n",
            "loss: 1.147536800647604\n",
            "training time 0h 2m 47s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 4\n",
            "loss: 1.1643384738215086\n",
            "training time 0h 2m 51s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 5\n",
            "loss: 1.2008490467893667\n",
            "training time 0h 2m 51s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 6\n",
            "loss: 1.2465819219063068\n",
            "training time 0h 2m 52s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 7\n",
            "loss: 1.3459135581707131\n",
            "training time 0h 2m 52s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 8\n",
            "loss: 1.3529976561151702\n",
            "training time 0h 2m 52s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 9\n",
            "loss: 1.321406245642695\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 10\n",
            "loss: 1.3309821527579735\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnZ1BQwFt8vA"
      },
      "source": [
        "**Generating Tokenized BERT embeddings for tokens in vocab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAK1yeSNmcn7",
        "outputId": "9758ad31-66c8-4a77-cd7b-b4d2fecbf915"
      },
      "source": [
        "import json\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "import torch\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "\n",
        "vocab = {}\n",
        "emb_dict = {}\n",
        "with open(\"vocab_dict.json\",'r') as f:\n",
        "  vocab = json.load(f)\n",
        "\n",
        "words_list = list(vocab.keys())\n",
        "\n",
        "print(words_list[0])\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
        "\n",
        "BertModel = BertModel.from_pretrained('bert-base-chinese').to(device)\n",
        "BertModel.eval()\n",
        "\n",
        "print(len(words_list))\n",
        "for idx in range(2):\n",
        "  print(idx)\n",
        "  cap = u'[CLS] '+words_list[idx]\n",
        "                  \n",
        "  if cap != '\\u3000' and cap != '':\n",
        "    tokenized_cap = tokenizer.tokenize(cap)                \n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_cap)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      encoded_layers, _ = BertModel(tokens_tensor)\n",
        "\n",
        "    bert_embedding = encoded_layers[11].squeeze(0)\n",
        "\n",
        "    print(bert_embedding.shape)\n",
        "\n",
        "    dim = bert_embedding.shape[0]\n",
        "\n",
        "    if dim > 1:\n",
        "      x = bert_embedding[0]\n",
        "      x = x.reshape(1,768)\n",
        "      for i in range(1,dim):\n",
        "        temp_emb = bert_embedding[i]\n",
        "        temp_emb = temp_emb.reshape(1,768)\n",
        "        x = torch.add(x,temp_emb)\n",
        "    else:\n",
        "      x = bert_embedding[0]\n",
        "      x = x.reshape(1,768)\n",
        "\n",
        "    x = x.to(\"cpu\")\n",
        "    x_np = x.data.numpy()\n",
        "  else:\n",
        "    x_np = np.random.randn(1, 768)\n",
        "\n",
        "  emb_dict[words_list[idx]] = x_np\n",
        "\n",
        "with open('embeddings_dict.pickle', 'wb') as handle:\n",
        "    pickle.dump(emb_dict, handle)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PAD\n",
            "3690\n",
            "0\n",
            "torch.Size([2, 768])\n",
            "1\n",
            "torch.Size([3, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40yE4a7-2wI_",
        "outputId": "eb39af3a-2e40-4ca1-d90a-828bbe78a530"
      },
      "source": [
        "#Embedding size = 768, preloaded embeddings from BERT \n",
        "!python3 run_seq2tree.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Transfer numbers...\n",
            "(['镇海', '雅乐', '学校', '二年级', '的', '小朋友', '到', '一条', '小路', '的', '一边', '植树', '．', '小朋友', '们', '每隔', 'NUM', '米', '种', '一棵树', '（', '马路', '两头', '都', '种', '了', '树', '）', '，', '最后', '发现', '一共', '种', '了', 'NUM', '棵', '，', '这', '条', '小路', '长', '多少', '米', '．'], ['(', 'N1', '-', '1', ')', '*', 'N0'], ['2', '11'], [16, 34])\n",
            "Indexing words...\n",
            "keep_words 3671 / 9971 = 0.3682\n",
            "Indexed 3674 words in input language, 23 words in output\n",
            "Number of training data 18530\n",
            "Number of testind data 4632\n",
            "Vocab Saved\n",
            "TIL: 3674\n",
            "([3, 4, 5, 1, 6, 7, 8, 9, 10, 11, 12, 13, 1, 14, 10, 15, 1, 16, 17, 2, 10, 18, 19, 20, 21, 16, 22], 27, [0, 9, 8], 3, ['5', '0.8', '150'], [3, 12, 16], [])\n",
            "Input size: 3674\n",
            "Embeddings Loaded\n",
            "fold: 1\n",
            "epoch: 1\n",
            "loss: 1.4425452291965484\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "1050 1164 4632\n",
            "test_answer_acc 0.2266839378238342 0.25129533678756477\n",
            "testing time 0h 4m 40s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 2\n",
            "loss: 0.8969535687874104\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 3\n",
            "loss: 0.7080728388038174\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 4\n",
            "loss: 0.6138735491653968\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 5\n",
            "loss: 0.5555450292496845\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 6\n",
            "loss: 0.511797789664104\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 7\n",
            "loss: 0.4801867713188303\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 8\n",
            "loss: 0.44825397082443896\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 9\n",
            "loss: 0.42775756619099914\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 10\n",
            "loss: 0.40643571800199046\n",
            "training time 0h 2m 46s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 11\n",
            "loss: 0.38676234740635446\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "2686 3086 4632\n",
            "test_answer_acc 0.5798791018998273 0.6662348877374784\n",
            "testing time 0h 5m 38s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 12\n",
            "loss: 0.3772192849681295\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 13\n",
            "loss: 0.35993127247382856\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 14\n",
            "loss: 0.34608072838906584\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 15\n",
            "loss: 0.3407778064238614\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 16\n",
            "loss: 0.3255571877648091\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 17\n",
            "loss: 0.31514448415616464\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "2772 3156 4632\n",
            "test_answer_acc 0.5984455958549223 0.6813471502590673\n",
            "testing time 0h 5m 42s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 18\n",
            "loss: 0.3104548096142966\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "2803 3212 4632\n",
            "test_answer_acc 0.6051381692573402 0.6934369602763385\n",
            "testing time 0h 5m 37s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 19\n",
            "loss: 0.2998654699017262\n",
            "training time 0h 2m 51s\n",
            "--------------------------------\n",
            "2790 3185 4632\n",
            "test_answer_acc 0.6023316062176166 0.687607944732297\n",
            "testing time 0h 5m 55s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 20\n",
            "loss: 0.28817566083423024\n",
            "training time 0h 2m 56s\n",
            "--------------------------------\n",
            "2773 3202 4632\n",
            "test_answer_acc 0.5986614853195165 0.6912780656303973\n",
            "testing time 0h 6m 11s\n",
            "------------------------------------------------------\n",
            "Indexing words...\n",
            "keep_words 3687 / 10014 = 0.3682\n",
            "Indexed 3690 words in input language, 23 words in output\n",
            "Number of training data 18530\n",
            "Number of testind data 4632\n",
            "Vocab Saved\n",
            "TIL: 3690\n",
            "([2, 2, 3, 4, 5, 6, 7, 8, 9, 5, 10, 11, 12, 6, 13, 14, 1, 15, 16, 17, 18, 19, 20, 21, 16, 22, 23, 24, 25, 26, 27, 28, 16, 22, 1, 29, 25, 30, 31, 9, 32, 33, 15, 12], 44, [0, 1, 8, 5, 7], 5, ['2', '11'], [16, 34], [])\n",
            "Input size: 3690\n",
            "Embeddings Loaded\n",
            "fold: 2\n",
            "epoch: 1\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [24,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:362: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexType = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2, IndexIsMajor = true]: block: [153,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"run_seq2tree.py\", line 103, in <module>\n",
            "    encoder_optimizer, predict_optimizer, generate_optimizer, merge_optimizer, output_lang, num_pos_batches[idx])\n",
            "  File \"/content/gdrive/My Drive/math_seq2tree/src/train_and_evaluate.py\", line 700, in train_tree\n",
            "    encoder.hidden_size)\n",
            "  File \"/content/gdrive/My Drive/math_seq2tree/src/train_and_evaluate.py\", line 372, in get_all_number_encoder_outputs\n",
            "    indices = indices.cuda()\n",
            "RuntimeError: CUDA error: device-side assert triggered\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aNUchT138Kl",
        "outputId": "71ba2c51-7b36-4bb6-d31e-bb0337cc532a"
      },
      "source": [
        "#Embedding size = 768, preloaded embeddings from BERT (Run 2) Epochs 20\n",
        "!python3 run_seq2tree.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Transfer numbers...\n",
            "(['镇海', '雅乐', '学校', '二年级', '的', '小朋友', '到', '一条', '小路', '的', '一边', '植树', '．', '小朋友', '们', '每隔', 'NUM', '米', '种', '一棵树', '（', '马路', '两头', '都', '种', '了', '树', '）', '，', '最后', '发现', '一共', '种', '了', 'NUM', '棵', '，', '这', '条', '小路', '长', '多少', '米', '．'], ['(', 'N1', '-', '1', ')', '*', 'N0'], ['2', '11'], [16, 34])\n",
            "Indexing words...\n",
            "keep_words 3671 / 9971 = 0.3682\n",
            "Indexed 3674 words in input language, 23 words in output\n",
            "Number of training data 18530\n",
            "Number of testind data 4632\n",
            "Vocab Saved\n",
            "TIL: 3674\n",
            "PAD\n",
            "3674\n",
            "([3, 4, 5, 1, 6, 7, 8, 9, 10, 11, 12, 13, 1, 14, 10, 15, 1, 16, 17, 2, 10, 18, 19, 20, 21, 16, 22], 27, [0, 9, 8], 3, ['5', '0.8', '150'], [3, 12, 16], [])\n",
            "Input size: 3674\n",
            "Embeddings Loaded\n",
            "fold: 1\n",
            "epoch: 1\n",
            "loss: 1.38668848193925\n",
            "training time 0h 2m 29s\n",
            "--------------------------------\n",
            "1289 1424 4632\n",
            "test_answer_acc 0.27828151986183075 0.307426597582038\n",
            "testing time 0h 4m 28s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 2\n",
            "loss: 0.8449282921593765\n",
            "training time 0h 2m 24s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 3\n",
            "loss: 0.6804182358856858\n",
            "training time 0h 2m 23s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 4\n",
            "loss: 0.5996696224500393\n",
            "training time 0h 2m 23s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 5\n",
            "loss: 0.5345078988321896\n",
            "training time 0h 2m 24s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 6\n",
            "loss: 0.4955227131473607\n",
            "training time 0h 2m 22s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 7\n",
            "loss: 0.4571817905738436\n",
            "training time 0h 2m 21s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 8\n",
            "loss: 0.4274590616596156\n",
            "training time 0h 2m 23s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 9\n",
            "loss: 0.4079904231531867\n",
            "training time 0h 2m 26s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 10\n",
            "loss: 0.3854860002624578\n",
            "training time 0h 2m 28s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 11\n",
            "loss: 0.37291298056470934\n",
            "training time 0h 2m 31s\n",
            "--------------------------------\n",
            "2720 3098 4632\n",
            "test_answer_acc 0.5872193436960277 0.668825561312608\n",
            "testing time 0h 5m 11s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 12\n",
            "loss: 0.3556350208561996\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 13\n",
            "loss: 0.3427810436178898\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 14\n",
            "loss: 0.32849660330805286\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 15\n",
            "loss: 0.31770724032459585\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 16\n",
            "loss: 0.3071640536189079\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 17\n",
            "loss: 0.2987057232137384\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "2750 3133 4632\n",
            "test_answer_acc 0.5936960276338514 0.6763816925734024\n",
            "testing time 0h 5m 12s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 18\n",
            "loss: 0.28950409909774516\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "2760 3173 4632\n",
            "test_answer_acc 0.5958549222797928 0.6850172711571675\n",
            "testing time 0h 5m 8s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 19\n",
            "loss: 0.2863687042532296\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "2751 3184 4632\n",
            "test_answer_acc 0.5939119170984456 0.687392055267703\n",
            "testing time 0h 5m 27s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 20\n",
            "loss: 0.27637652107353866\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "2771 3150 4632\n",
            "test_answer_acc 0.5982297063903281 0.6800518134715026\n",
            "testing time 0h 5m 6s\n",
            "------------------------------------------------------\n",
            "Indexing words...\n",
            "keep_words 3687 / 10014 = 0.3682\n",
            "Indexed 3690 words in input language, 23 words in output\n",
            "Number of training data 18530\n",
            "Number of testind data 4632\n",
            "Vocab Saved\n",
            "TIL: 3690\n",
            "PAD\n",
            "3690\n",
            "([2, 2, 3, 4, 5, 6, 7, 8, 9, 5, 10, 11, 12, 6, 13, 14, 1, 15, 16, 17, 18, 19, 20, 21, 16, 22, 23, 24, 25, 26, 27, 28, 16, 22, 1, 29, 25, 30, 31, 9, 32, 33, 15, 12], 44, [0, 1, 8, 5, 7], 5, ['2', '11'], [16, 34], [])\n",
            "Input size: 3690\n",
            "Embeddings Loaded\n",
            "fold: 2\n",
            "epoch: 1\n",
            "loss: 1.3754848426785962\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "1378 1510 4632\n",
            "test_answer_acc 0.2974956822107081 0.32599309153713296\n",
            "testing time 0h 5m 2s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 2\n",
            "loss: 0.849808677928201\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 3\n",
            "loss: 0.6885793658166096\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 4\n",
            "loss: 0.5989388189439115\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 5\n",
            "loss: 0.5398961824589762\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 6\n",
            "loss: 0.4981847468121298\n",
            "training time 0h 2m 31s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 7\n",
            "loss: 0.4605868689972779\n",
            "training time 0h 2m 29s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 8\n",
            "loss: 0.43208658109451165\n",
            "training time 0h 2m 32s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 9\n",
            "loss: 0.4075812098281137\n",
            "training time 0h 2m 32s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 10\n",
            "loss: 0.39000976840997564\n",
            "training time 0h 2m 32s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 11\n",
            "loss: 0.3720854969887898\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "2761 3133 4632\n",
            "test_answer_acc 0.5960708117443869 0.6763816925734024\n",
            "testing time 0h 5m 13s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 12\n",
            "loss: 0.35546970028301766\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 13\n",
            "loss: 0.34435508446446783\n",
            "training time 0h 2m 31s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 14\n",
            "loss: 0.332314938717875\n",
            "training time 0h 2m 33s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 15\n",
            "loss: 0.3225664828358025\n",
            "training time 0h 2m 33s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 16\n",
            "loss: 0.31228358725021627\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 17\n",
            "loss: 0.3053654714391149\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "2814 3198 4632\n",
            "test_answer_acc 0.6075129533678757 0.6904145077720207\n",
            "testing time 0h 4m 56s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 18\n",
            "loss: 0.2935216982816828\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "2808 3222 4632\n",
            "test_answer_acc 0.6062176165803109 0.6955958549222798\n",
            "testing time 0h 5m 24s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 19\n",
            "loss: 0.2869717331282024\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "2854 3273 4632\n",
            "test_answer_acc 0.6161485319516408 0.7066062176165803\n",
            "testing time 0h 5m 4s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 20\n",
            "loss: 0.2792636083117847\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "2785 3175 4632\n",
            "test_answer_acc 0.6012521588946459 0.6854490500863558\n",
            "testing time 0h 5m 7s\n",
            "------------------------------------------------------\n",
            "Indexing words...\n",
            "keep_words 3675 / 10000 = 0.3675\n",
            "Indexed 3678 words in input language, 23 words in output\n",
            "Number of training data 18530\n",
            "Number of testind data 4632\n",
            "Vocab Saved\n",
            "TIL: 3678\n",
            "PAD\n",
            "3678\n",
            "([2, 2, 3, 4, 5, 6, 7, 8, 9, 5, 10, 11, 12, 6, 13, 14, 1, 15, 16, 17, 18, 19, 20, 21, 16, 22, 23, 24, 25, 26, 27, 28, 16, 22, 1, 29, 25, 30, 31, 9, 32, 33, 15, 12], 44, [0, 1, 8, 5, 7], 5, ['2', '11'], [16, 34], [])\n",
            "Input size: 3678\n",
            "Embeddings Loaded\n",
            "fold: 3\n",
            "epoch: 1\n",
            "loss: 1.3884803196479534\n",
            "training time 0h 2m 33s\n",
            "--------------------------------\n",
            "1231 1396 4632\n",
            "test_answer_acc 0.2657599309153713 0.3013816925734024\n",
            "testing time 0h 4m 13s\n",
            "------------------------------------------------------\n",
            "fold: 3\n",
            "epoch: 2\n",
            "loss: 0.8414224086136654\n",
            "training time 0h 2m 31s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 3\n",
            "loss: 0.6773144223566713\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 4\n",
            "loss: 0.5926448395539974\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 5\n",
            "loss: 0.5341457050422143\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 6\n",
            "loss: 0.4879218400552355\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 7\n",
            "loss: 0.4612408328672935\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 8\n",
            "loss: 0.43014962662910594\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 9\n",
            "loss: 0.4026786847875036\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 10\n",
            "loss: 0.38660906332320183\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 11\n",
            "loss: 0.36828635823110056\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "2694 3114 4632\n",
            "test_answer_acc 0.5816062176165803 0.6722797927461139\n",
            "testing time 0h 5m 17s\n",
            "------------------------------------------------------\n",
            "fold: 3\n",
            "epoch: 12\n",
            "loss: 0.3490088727453659\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 13\n",
            "loss: 0.33807579916099023\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 14\n",
            "loss: 0.32769646721667256\n",
            "training time 0h 2m 33s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 15\n",
            "loss: 0.31654861852012833\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 16\n",
            "loss: 0.30496949188668154\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 17\n",
            "loss: 0.2971584736273207\n",
            "training time 0h 2m 33s\n",
            "--------------------------------\n",
            "2762 3184 4632\n",
            "test_answer_acc 0.596286701208981 0.687392055267703\n",
            "testing time 0h 5m 15s\n",
            "------------------------------------------------------\n",
            "fold: 3\n",
            "epoch: 18\n",
            "loss: 0.29053744082820826\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "2794 3232 4632\n",
            "test_answer_acc 0.603195164075993 0.697754749568221\n",
            "testing time 0h 5m 23s\n",
            "------------------------------------------------------\n",
            "fold: 3\n",
            "epoch: 19\n",
            "loss: 0.28204992850278987\n",
            "training time 0h 2m 34s\n",
            "--------------------------------\n",
            "2803 3243 4632\n",
            "test_answer_acc 0.6051381692573402 0.7001295336787565\n",
            "testing time 0h 5m 26s\n",
            "------------------------------------------------------\n",
            "fold: 3\n",
            "epoch: 20\n",
            "loss: 0.27317396628445595\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "2789 3224 4632\n",
            "test_answer_acc 0.6021157167530224 0.696027633851468\n",
            "testing time 0h 5m 18s\n",
            "------------------------------------------------------\n",
            "Indexing words...\n",
            "keep_words 3699 / 9984 = 0.3705\n",
            "Indexed 3702 words in input language, 23 words in output\n",
            "Number of training data 18530\n",
            "Number of testind data 4632\n",
            "Vocab Saved\n",
            "TIL: 3702\n",
            "PAD\n",
            "3702\n",
            "([2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 11, 12, 13, 7, 14, 15, 1, 16, 17, 18, 19, 20, 21, 22, 17, 23, 24, 25, 26, 27, 28, 29, 17, 23, 1, 30, 26, 31, 32, 10, 33, 34, 16, 13], 44, [0, 1, 8, 5, 7], 5, ['2', '11'], [16, 34], [])\n",
            "Input size: 3702\n",
            "Embeddings Loaded\n",
            "fold: 4\n",
            "epoch: 1\n",
            "loss: 1.3747654984737265\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "1340 1486 4632\n",
            "test_answer_acc 0.28929188255613125 0.32081174438687393\n",
            "testing time 0h 5m 39s\n",
            "------------------------------------------------------\n",
            "fold: 4\n",
            "epoch: 2\n",
            "loss: 0.8456663861356932\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 4\n",
            "epoch: 3\n",
            "loss: 0.6792674910405586\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 4\n",
            "epoch: 4\n",
            "loss: 0.5926084880171151\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 4\n",
            "epoch: 5\n",
            "loss: 0.5352211425016666\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "fold: 4\n",
            "epoch: 6\n",
            "loss: 0.48910949209640764\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "fold: 4\n",
            "epoch: 7\n",
            "loss: 0.46200184380186016\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 4\n",
            "epoch: 8\n",
            "loss: 0.42827495593449166\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "fold: 4\n",
            "epoch: 9\n",
            "loss: 0.4040975012142083\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "fold: 4\n",
            "epoch: 10\n",
            "loss: 0.3848429412163537\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "fold: 4\n",
            "epoch: 11\n",
            "loss: 0.3675191115202575\n",
            "training time 0h 2m 35s\n",
            "--------------------------------\n",
            "2689 3098 4632\n",
            "test_answer_acc 0.5805267702936097 0.668825561312608\n",
            "testing time 0h 5m 26s\n",
            "------------------------------------------------------\n",
            "fold: 4\n",
            "epoch: 12\n",
            "loss: 0.3502456130138759\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "fold: 4\n",
            "epoch: 13\n",
            "loss: 0.33932928160346787\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 4\n",
            "epoch: 14\n",
            "loss: 0.3301268138762178\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "fold: 4\n",
            "epoch: 15\n",
            "loss: 0.3151968859393021\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "fold: 4\n",
            "epoch: 16\n",
            "loss: 0.306779793264537\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "fold: 4\n",
            "epoch: 17\n",
            "loss: 0.29915906319330476\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "2765 3172 4632\n",
            "test_answer_acc 0.5969343696027634 0.6848013816925734\n",
            "testing time 0h 5m 4s\n",
            "------------------------------------------------------\n",
            "fold: 4\n",
            "epoch: 18\n",
            "loss: 0.29445532966276694\n",
            "training time 0h 2m 36s\n",
            "--------------------------------\n",
            "2752 3158 4632\n",
            "test_answer_acc 0.5941278065630398 0.6817789291882557\n",
            "testing time 0h 5m 21s\n",
            "------------------------------------------------------\n",
            "fold: 4\n",
            "epoch: 19\n",
            "loss: 0.2832355815788795\n",
            "training time 0h 2m 32s\n",
            "--------------------------------\n",
            "2752 3164 4632\n",
            "test_answer_acc 0.5941278065630398 0.6830742659758203\n",
            "testing time 0h 5m 3s\n",
            "------------------------------------------------------\n",
            "fold: 4\n",
            "epoch: 20\n",
            "loss: 0.2776501914550518\n",
            "training time 0h 2m 25s\n",
            "--------------------------------\n",
            "2763 3178 4632\n",
            "test_answer_acc 0.5965025906735751 0.6860967184801382\n",
            "testing time 0h 4m 48s\n",
            "------------------------------------------------------\n",
            "Indexing words...\n",
            "keep_words 3690 / 9962 = 0.3704\n",
            "Indexed 3693 words in input language, 23 words in output\n",
            "Number of training data 18528\n",
            "Number of testind data 4634\n",
            "Vocab Saved\n",
            "TIL: 3693\n",
            "PAD\n",
            "3693\n",
            "([2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 11, 12, 13, 7, 14, 15, 1, 16, 17, 18, 19, 20, 21, 22, 17, 23, 24, 25, 26, 27, 28, 29, 17, 23, 1, 30, 26, 31, 32, 10, 33, 34, 16, 13], 44, [0, 1, 8, 5, 7], 5, ['2', '11'], [16, 34], [])\n",
            "Input size: 3693\n",
            "Embeddings Loaded\n",
            "fold: 5\n",
            "epoch: 1\n",
            "loss: 1.3832945944934056\n",
            "training time 0h 2m 24s\n",
            "--------------------------------\n",
            "1477 1645 4634\n",
            "test_answer_acc 0.3187311178247734 0.3549848942598187\n",
            "testing time 0h 5m 21s\n",
            "------------------------------------------------------\n",
            "fold: 5\n",
            "epoch: 2\n",
            "loss: 0.8417074875584964\n",
            "training time 0h 2m 24s\n",
            "--------------------------------\n",
            "fold: 5\n",
            "epoch: 3\n",
            "loss: 0.6823444978944186\n",
            "training time 0h 2m 25s\n",
            "--------------------------------\n",
            "fold: 5\n",
            "epoch: 4\n",
            "loss: 0.5924404469029657\n",
            "training time 0h 2m 24s\n",
            "--------------------------------\n",
            "fold: 5\n",
            "epoch: 5\n",
            "loss: 0.5320658895476111\n",
            "training time 0h 2m 24s\n",
            "--------------------------------\n",
            "fold: 5\n",
            "epoch: 6\n",
            "loss: 0.49103005867579885\n",
            "training time 0h 2m 23s\n",
            "--------------------------------\n",
            "fold: 5\n",
            "epoch: 7\n",
            "loss: 0.4605885955794104\n",
            "training time 0h 2m 24s\n",
            "--------------------------------\n",
            "fold: 5\n",
            "epoch: 8\n",
            "loss: 0.43348542780711735\n",
            "training time 0h 2m 26s\n",
            "--------------------------------\n",
            "fold: 5\n",
            "epoch: 9\n",
            "loss: 0.40801714422373936\n",
            "training time 0h 2m 29s\n",
            "--------------------------------\n",
            "fold: 5\n",
            "epoch: 10\n",
            "loss: 0.38512411091862053\n",
            "training time 0h 2m 31s\n",
            "--------------------------------\n",
            "fold: 5\n",
            "epoch: 11\n",
            "loss: 0.37099012552664196\n",
            "training time 0h 2m 27s\n",
            "--------------------------------\n",
            "2682 3091 4634\n",
            "test_answer_acc 0.578765645230902 0.667026327147173\n",
            "testing time 0h 4m 54s\n",
            "------------------------------------------------------\n",
            "fold: 5\n",
            "epoch: 12\n",
            "loss: 0.3537057337575945\n",
            "training time 0h 2m 24s\n",
            "--------------------------------\n",
            "fold: 5\n",
            "epoch: 13\n",
            "loss: 0.3422131487521632\n",
            "training time 0h 2m 24s\n",
            "--------------------------------\n",
            "fold: 5\n",
            "epoch: 14\n",
            "loss: 0.32980019352559387\n",
            "training time 0h 2m 23s\n",
            "--------------------------------\n",
            "fold: 5\n",
            "epoch: 15\n",
            "loss: 0.3188978241949246\n",
            "training time 0h 2m 23s\n",
            "--------------------------------\n",
            "fold: 5\n",
            "epoch: 16\n",
            "loss: 0.31004582294102373\n",
            "training time 0h 2m 23s\n",
            "--------------------------------\n",
            "fold: 5\n",
            "epoch: 17\n",
            "loss: 0.29962266770930124\n",
            "training time 0h 2m 23s\n",
            "--------------------------------\n",
            "2727 3176 4634\n",
            "test_answer_acc 0.5884764782045748 0.6853690116529996\n",
            "testing time 0h 4m 46s\n",
            "------------------------------------------------------\n",
            "fold: 5\n",
            "epoch: 18\n",
            "loss: 0.2911021202288825\n",
            "training time 0h 2m 29s\n",
            "--------------------------------\n",
            "2769 3198 4634\n",
            "test_answer_acc 0.5975399223133362 0.6901165299956841\n",
            "testing time 0h 4m 45s\n",
            "------------------------------------------------------\n",
            "fold: 5\n",
            "epoch: 19\n",
            "loss: 0.28321348038213007\n",
            "training time 0h 2m 26s\n",
            "--------------------------------\n",
            "2751 3196 4634\n",
            "test_answer_acc 0.5936555891238671 0.6896849374190764\n",
            "testing time 0h 4m 46s\n",
            "------------------------------------------------------\n",
            "fold: 5\n",
            "epoch: 20\n",
            "loss: 0.28081643992456895\n",
            "training time 0h 2m 26s\n",
            "--------------------------------\n",
            "2771 3183 4634\n",
            "test_answer_acc 0.5979715148899439 0.6868795856711265\n",
            "testing time 0h 4m 39s\n",
            "------------------------------------------------------\n",
            "(2771, 3150, 4632)\n",
            "(2785, 3175, 4632)\n",
            "(2789, 3224, 4632)\n",
            "(2763, 3178, 4632)\n",
            "(2771, 3183, 4634)\n",
            "0.5992142302046456 0.6869009584664537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnQZ_AzuOKTm",
        "outputId": "953d98c9-8c41-4786-a813-380f4bf346fe"
      },
      "source": [
        "#Embedding size = 768, preloaded embeddings from BERT (Run 3) epochs 40\n",
        "!python3 run_seq2tree.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Transfer numbers...\n",
            "(['镇海', '雅乐', '学校', '二年级', '的', '小朋友', '到', '一条', '小路', '的', '一边', '植树', '．', '小朋友', '们', '每隔', 'NUM', '米', '种', '一棵树', '（', '马路', '两头', '都', '种', '了', '树', '）', '，', '最后', '发现', '一共', '种', '了', 'NUM', '棵', '，', '这', '条', '小路', '长', '多少', '米', '．'], ['(', 'N1', '-', '1', ')', '*', 'N0'], ['2', '11'], [16, 34])\n",
            "Indexing words...\n",
            "keep_words 3671 / 9971 = 0.3682\n",
            "Indexed 3674 words in input language, 23 words in output\n",
            "Number of training data 18530\n",
            "Number of testind data 4632\n",
            "Vocab Saved\n",
            "TIL: 3674\n",
            "PAD\n",
            "100% 109540/109540 [00:00<00:00, 575587.92B/s]\n",
            "100% 382072689/382072689 [00:09<00:00, 38913045.08B/s]\n",
            "3674\n",
            "([3, 4, 5, 1, 6, 7, 8, 9, 10, 11, 12, 13, 1, 14, 10, 15, 1, 16, 17, 2, 10, 18, 19, 20, 21, 16, 22], 27, [0, 9, 8], 3, ['5', '0.8', '150'], [3, 12, 16], [])\n",
            "Input size: 3674\n",
            "Embeddings Loaded\n",
            "fold: 1\n",
            "epoch: 1\n",
            "loss: 1.372649154169806\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "1360 1500 4632\n",
            "test_answer_acc 0.29360967184801384 0.3238341968911917\n",
            "testing time 0h 5m 10s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 2\n",
            "loss: 0.8419557117182633\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 3\n",
            "loss: 0.6811524294573685\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 4\n",
            "loss: 0.5940132150362278\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 5\n",
            "loss: 0.53494569905873\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 6\n",
            "loss: 0.4939070208319302\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 7\n",
            "loss: 0.46229619712665165\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 8\n",
            "loss: 0.4321794702061291\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 9\n",
            "loss: 0.40477326990201556\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 10\n",
            "loss: 0.3933311140742795\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 11\n",
            "loss: 0.3720977810436282\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "2751 3124 4632\n",
            "test_answer_acc 0.5939119170984456 0.6744386873920553\n",
            "testing time 0h 5m 33s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 12\n",
            "loss: 0.35735522534312875\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 13\n",
            "loss: 0.34049941162610875\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 14\n",
            "loss: 0.3333253221265201\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 15\n",
            "loss: 0.3267992458980659\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 16\n",
            "loss: 0.30816128022711853\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 17\n",
            "loss: 0.302872796356678\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 18\n",
            "loss: 0.2965318169573258\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 19\n",
            "loss: 0.2894198083158197\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 20\n",
            "loss: 0.2783322348676879\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 21\n",
            "loss: 0.23522592433567704\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "2825 3279 4632\n",
            "test_answer_acc 0.6098877374784111 0.707901554404145\n",
            "testing time 0h 5m 27s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 22\n",
            "loss: 0.21024814612906553\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 23\n",
            "loss: 0.19605572657852338\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 24\n",
            "loss: 0.18438178192952584\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 25\n",
            "loss: 0.17646718315523247\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 26\n",
            "loss: 0.17105138062916952\n",
            "training time 0h 2m 46s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 27\n",
            "loss: 0.16756033571115855\n",
            "training time 0h 2m 49s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 28\n",
            "loss: 0.16335287302218635\n",
            "training time 0h 2m 50s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 29\n",
            "loss: 0.15949660984092745\n",
            "training time 0h 2m 51s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 30\n",
            "loss: 0.15568605574040578\n",
            "training time 0h 2m 50s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 31\n",
            "loss: 0.14807818200567674\n",
            "training time 0h 2m 52s\n",
            "--------------------------------\n",
            "2824 3306 4632\n",
            "test_answer_acc 0.6096718480138169 0.7137305699481865\n",
            "testing time 0h 5m 51s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 32\n",
            "loss: 0.14750519739142778\n",
            "training time 0h 2m 54s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 33\n",
            "loss: 0.14125577984698887\n",
            "training time 0h 2m 54s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 34\n",
            "loss: 0.13960670571902703\n",
            "training time 0h 2m 54s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 35\n",
            "loss: 0.13861283797385363\n",
            "training time 0h 2m 52s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 36\n",
            "loss: 0.13537059123146122\n",
            "training time 0h 2m 54s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 37\n",
            "loss: 0.1341763821141473\n",
            "training time 0h 2m 53s\n",
            "--------------------------------\n",
            "2877 3334 4632\n",
            "test_answer_acc 0.6211139896373057 0.7197754749568221\n",
            "testing time 0h 5m 45s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 38\n",
            "loss: 0.12921654578169872\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "2846 3321 4632\n",
            "test_answer_acc 0.6144214162348878 0.7169689119170984\n",
            "testing time 0h 5m 35s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 39\n",
            "loss: 0.12952079049729068\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "2843 3321 4632\n",
            "test_answer_acc 0.6137737478411054 0.7169689119170984\n",
            "testing time 0h 5m 17s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 40\n",
            "loss: 0.12587631488668508\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "2829 3299 4632\n",
            "test_answer_acc 0.6107512953367875 0.7122193436960277\n",
            "testing time 0h 5m 12s\n",
            "------------------------------------------------------\n",
            "Indexing words...\n",
            "keep_words 3687 / 10014 = 0.3682\n",
            "Indexed 3690 words in input language, 23 words in output\n",
            "Number of training data 18530\n",
            "Number of testind data 4632\n",
            "Vocab Saved\n",
            "TIL: 3690\n",
            "PAD\n",
            "3690\n",
            "([2, 2, 3, 4, 5, 6, 7, 8, 9, 5, 10, 11, 12, 6, 13, 14, 1, 15, 16, 17, 18, 19, 20, 21, 16, 22, 23, 24, 25, 26, 27, 28, 16, 22, 1, 29, 25, 30, 31, 9, 32, 33, 15, 12], 44, [0, 1, 8, 5, 7], 5, ['2', '11'], [16, 34], [])\n",
            "Input size: 3690\n",
            "Embeddings Loaded\n",
            "fold: 2\n",
            "epoch: 1\n",
            "loss: 1.3874095291926942\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "1321 1437 4632\n",
            "test_answer_acc 0.28518998272884283 0.31023316062176165\n",
            "testing time 0h 4m 41s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 2\n",
            "loss: 0.8456341994219813\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 3\n",
            "loss: 0.6836372316911303\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 4\n",
            "loss: 0.5962851446250389\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 5\n",
            "loss: 0.5357288337987045\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 6\n",
            "loss: 0.4959652432079973\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 7\n",
            "loss: 0.4600859262819948\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 8\n",
            "loss: 0.4322342677876867\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 9\n",
            "loss: 0.41141447907891765\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 10\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKpglxFq5nlP",
        "outputId": "c991dad1-ea42-4ec0-85a9-459d94ff49e6"
      },
      "source": [
        "#Embedding size = 768, preloaded both input and output embeddings from BERT epochs 80\n",
        "!python3 run_seq2tree.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Transfer numbers...\n",
            "(['镇海', '雅乐', '学校', '二年级', '的', '小朋友', '到', '一条', '小路', '的', '一边', '植树', '．', '小朋友', '们', '每隔', 'NUM', '米', '种', '一棵树', '（', '马路', '两头', '都', '种', '了', '树', '）', '，', '最后', '发现', '一共', '种', '了', 'NUM', '棵', '，', '这', '条', '小路', '长', '多少', '米', '．'], ['(', 'N1', '-', '1', ')', '*', 'N0'], ['2', '11'], [16, 34])\n",
            "Indexing words...\n",
            "keep_words 3671 / 9971 = 0.3682\n",
            "Indexed 3674 words in input language, 23 words in output\n",
            "Number of training data 18530\n",
            "Number of testind data 4632\n",
            "Vocabs Saved\n",
            "TIL: 3674\n",
            "PAD\n",
            "3674\n",
            "*\n",
            "23\n",
            "([3, 4, 5, 1, 6, 7, 8, 9, 10, 11, 12, 13, 1, 14, 10, 15, 1, 16, 17, 2, 10, 18, 19, 20, 21, 16, 22], 27, [0, 9, 8], 3, ['5', '0.8', '150'], [3, 12, 16], [])\n",
            "Input size: 3674\n",
            " Input Embeddings Loaded\n",
            "Output Embeddings Loaded\n",
            "fold: 1\n",
            "epoch: 1\n",
            "loss: 1.3815670584810191\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "1232 1360 4632\n",
            "test_answer_acc 0.2659758203799655 0.29360967184801384\n",
            "testing time 0h 5m 8s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 2\n",
            "loss: 0.8392965183175843\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 3\n",
            "loss: 0.6769072456606503\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 4\n",
            "loss: 0.5911746997257759\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 5\n",
            "loss: 0.5299486706996787\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 6\n",
            "loss: 0.48888160288333893\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 7\n",
            "loss: 0.4565991389340368\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 8\n",
            "loss: 0.4300853297114372\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 9\n",
            "loss: 0.4068030660008562\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 10\n",
            "loss: 0.385831705403739\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 11\n",
            "loss: 0.36825339891787234\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "2691 3070 4632\n",
            "test_answer_acc 0.5809585492227979 0.6627806563039723\n",
            "testing time 0h 5m 25s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 12\n",
            "loss: 0.355056401008162\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 13\n",
            "loss: 0.3458406256190662\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 14\n",
            "loss: 0.3297045595687011\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 15\n",
            "loss: 0.3195061722192271\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 16\n",
            "loss: 0.309082816232895\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 17\n",
            "loss: 0.3020340312143852\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 18\n",
            "loss: 0.29550245039421935\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 19\n",
            "loss: 0.2832765165074118\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 20\n",
            "loss: 0.27848739187265265\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 21\n",
            "loss: 0.2381555978594155\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "2833 3290 4632\n",
            "test_answer_acc 0.6116148531951641 0.7102763385146805\n",
            "testing time 0h 5m 27s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 22\n",
            "loss: 0.20891631723477921\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 23\n",
            "loss: 0.19326218068599701\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 24\n",
            "loss: 0.18446003265421965\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 25\n",
            "loss: 0.17871523168066453\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 26\n",
            "loss: 0.17196921418453084\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 27\n",
            "loss: 0.16499705016613006\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 28\n",
            "loss: 0.16178737999550227\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 29\n",
            "loss: 0.1606461960950802\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 30\n",
            "loss: 0.1540081675967266\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 31\n",
            "loss: 0.14955313115798194\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "2859 3316 4632\n",
            "test_answer_acc 0.6172279792746114 0.7158894645941278\n",
            "testing time 0h 5m 21s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 32\n",
            "loss: 0.14694567011861964\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 33\n",
            "loss: 0.14649206590035865\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 34\n",
            "loss: 0.14224979375200025\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 35\n",
            "loss: 0.1397229858898911\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 36\n",
            "loss: 0.13302284147718857\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 37\n",
            "loss: 0.13235543526965995\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 38\n",
            "loss: 0.13274785693863342\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 39\n",
            "loss: 0.13146697654035586\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 40\n",
            "loss: 0.12601940201531198\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 41\n",
            "loss: 0.10858481986255482\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "2874 3352 4632\n",
            "test_answer_acc 0.6204663212435233 0.7236614853195165\n",
            "testing time 0h 5m 21s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 42\n",
            "loss: 0.09898912902792979\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 43\n",
            "loss: 0.09236023706094972\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 44\n",
            "loss: 0.09054521999225534\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 45\n",
            "loss: 0.08552283430793162\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 46\n",
            "loss: 0.08268448468940011\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 47\n",
            "loss: 0.08334230688111535\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 48\n",
            "loss: 0.08068310118697841\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 49\n",
            "loss: 0.07869785823698701\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 50\n",
            "loss: 0.07726692954913296\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 51\n",
            "loss: 0.07629551266416393\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "2870 3349 4632\n",
            "test_answer_acc 0.6196027633851469 0.7230138169257341\n",
            "testing time 0h 5m 18s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 52\n",
            "loss: 0.07284905652290788\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 53\n",
            "loss: 0.07311187798863855\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 54\n",
            "loss: 0.06962265454810755\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 55\n",
            "loss: 0.06945039403849634\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 56\n",
            "loss: 0.06939109267088873\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 57\n",
            "loss: 0.06941882162836605\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 58\n",
            "loss: 0.07040089375243105\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 59\n",
            "loss: 0.06614230964332819\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 60\n",
            "loss: 0.06516098724357013\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 61\n",
            "loss: 0.060786948108981394\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "2867 3350 4632\n",
            "test_answer_acc 0.6189550949913645 0.7232297063903281\n",
            "testing time 0h 5m 18s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 62\n",
            "loss: 0.05558592119286286\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 63\n",
            "loss: 0.05362492773424962\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 64\n",
            "loss: 0.05136365000608152\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 65\n",
            "loss: 0.0520284963611128\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 66\n",
            "loss: 0.05026130055976582\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 67\n",
            "loss: 0.047133855964860014\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 68\n",
            "loss: 0.047201276923818834\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 69\n",
            "loss: 0.047209101440063836\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 70\n",
            "loss: 0.04549834903265382\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 71\n",
            "loss: 0.04561989117927592\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "2867 3367 4632\n",
            "test_answer_acc 0.6189550949913645 0.7268998272884283\n",
            "testing time 0h 5m 22s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 72\n",
            "loss: 0.046090930077279435\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 73\n",
            "loss: 0.04500468895481578\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 74\n",
            "loss: 0.04350864684594603\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 75\n",
            "loss: 0.0433445088308433\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 76\n",
            "loss: 0.04306675270199776\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 77\n",
            "loss: 0.04265624324566331\n",
            "training time 0h 2m 37s\n",
            "--------------------------------\n",
            "2887 3385 4632\n",
            "test_answer_acc 0.623272884283247 0.7307858376511226\n",
            "testing time 0h 5m 12s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 78\n",
            "loss: 0.04106013062891775\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "2877 3372 4632\n",
            "test_answer_acc 0.6211139896373057 0.727979274611399\n",
            "testing time 0h 5m 18s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 79\n",
            "loss: 0.041972889275900246\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "2889 3381 4632\n",
            "test_answer_acc 0.6237046632124352 0.7299222797927462\n",
            "testing time 0h 5m 14s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 80\n",
            "loss: 0.0402407484675019\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "2877 3375 4632\n",
            "test_answer_acc 0.6211139896373057 0.7286269430051814\n",
            "testing time 0h 5m 41s\n",
            "------------------------------------------------------\n",
            "Indexing words...\n",
            "keep_words 3687 / 10014 = 0.3682\n",
            "Indexed 3690 words in input language, 23 words in output\n",
            "Number of training data 18530\n",
            "Number of testind data 4632\n",
            "Vocabs Saved\n",
            "TIL: 3690\n",
            "PAD\n",
            "3690\n",
            "*\n",
            "23\n",
            "([2, 2, 3, 4, 5, 6, 7, 8, 9, 5, 10, 11, 12, 6, 13, 14, 1, 15, 16, 17, 18, 19, 20, 21, 16, 22, 23, 24, 25, 26, 27, 28, 16, 22, 1, 29, 25, 30, 31, 9, 32, 33, 15, 12], 44, [0, 1, 8, 5, 7], 5, ['2', '11'], [16, 34], [])\n",
            "Input size: 3690\n",
            " Input Embeddings Loaded\n",
            "Output Embeddings Loaded\n",
            "fold: 2\n",
            "epoch: 1\n",
            "loss: 1.365668721034609\n",
            "training time 0h 2m 46s\n",
            "--------------------------------\n",
            "1413 1551 4632\n",
            "test_answer_acc 0.30505181347150256 0.33484455958549225\n",
            "testing time 0h 5m 15s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 2\n",
            "Traceback (most recent call last):\n",
            "  File \"run_seq2tree.py\", line 108, in <module>\n",
            "  File \"/content/gdrive/MyDrive/math_seq2tree/src/train_and_evaluate.py\", line 707, in train_tree\n",
            "    node_stacks, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden, seq_mask, num_mask)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 489, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/gdrive/MyDrive/math_seq2tree/src/models.py\", line 251, in forward\n",
            "    g = torch.tanh(self.concat_l(c))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 489, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\", line 67, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\", line 1352, in linear\n",
            "    ret = torch.addmm(torch.jit._unwrap_optional(bias), input, weight.t())\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQvUy1E-ZQvd",
        "outputId": "a3832765-af66-4afa-be40-622c48efaf0d"
      },
      "source": [
        "#Embedding size = 768,  preloaded both input and output embeddings from BERT epochs 80\n",
        "!python3 run_seq2tree.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Transfer numbers...\n",
            "(['镇海', '雅乐', '学校', '二年级', '的', '小朋友', '到', '一条', '小路', '的', '一边', '植树', '．', '小朋友', '们', '每隔', 'NUM', '米', '种', '一棵树', '（', '马路', '两头', '都', '种', '了', '树', '）', '，', '最后', '发现', '一共', '种', '了', 'NUM', '棵', '，', '这', '条', '小路', '长', '多少', '米', '．'], ['(', 'N1', '-', '1', ')', '*', 'N0'], ['2', '11'], [16, 34])\n",
            "Indexing words...\n",
            "keep_words 3671 / 9971 = 0.3682\n",
            "Indexed 3674 words in input language, 23 words in output\n",
            "Number of training data 18530\n",
            "Number of testind data 4632\n",
            "Vocabs Saved\n",
            "TIL: 3674\n",
            "PAD\n",
            "100% 109540/109540 [00:00<00:00, 16908109.53B/s]\n",
            "100% 382072689/382072689 [00:08<00:00, 46184584.26B/s]\n",
            "3674\n",
            "*\n",
            "23\n",
            "([3, 4, 5, 1, 6, 7, 8, 9, 10, 11, 12, 13, 1, 14, 10, 15, 1, 16, 17, 2, 10, 18, 19, 20, 21, 16, 22], 27, [0, 9, 8], 3, ['5', '0.8', '150'], [3, 12, 16], [])\n",
            "Input size: 3674\n",
            " Input Embeddings Loaded\n",
            "Output Embeddings Loaded\n",
            "fold: 1\n",
            "epoch: 1\n",
            "loss: 1.5628295610690939\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "682 746 4632\n",
            "test_answer_acc 0.14723661485319517 0.16105354058721935\n",
            "testing time 0h 5m 2s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 2\n",
            "loss: 0.9903126058907344\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 3\n",
            "loss: 0.7456361782961878\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 4\n",
            "loss: 0.6374550305563828\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 5\n",
            "loss: 0.560763771780606\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 6\n",
            "loss: 0.5092907850084634\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 7\n",
            "loss: 0.46359304333555285\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 8\n",
            "loss: 0.42915508130501057\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 9\n",
            "loss: 0.39544315769754607\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 10\n",
            "loss: 0.37220538032465966\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 11\n",
            "loss: 0.3497153998448931\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "2713 3107 4632\n",
            "test_answer_acc 0.5857081174438687 0.6707685664939551\n",
            "testing time 0h 5m 19s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 12\n",
            "loss: 0.3320390426907046\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 13\n",
            "loss: 0.31803481938510103\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 14\n",
            "loss: 0.2980319645898096\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 15\n",
            "loss: 0.28702101142242037\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 16\n",
            "loss: 0.275973900536011\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 17\n",
            "loss: 0.26234606576376945\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 18\n",
            "loss: 0.255823683327642\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 19\n",
            "loss: 0.24670940113478693\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 20\n",
            "loss: 0.23544195910979962\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 21\n",
            "loss: 0.1952396177012345\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "2856 3315 4632\n",
            "test_answer_acc 0.616580310880829 0.7156735751295337\n",
            "testing time 0h 5m 17s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 22\n",
            "loss: 0.1703251948130542\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 23\n",
            "loss: 0.15965556718152144\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 24\n",
            "loss: 0.1520815887841685\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 25\n",
            "loss: 0.14452832731707344\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 26\n",
            "loss: 0.14255976250459407\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 27\n",
            "loss: 0.1376813630091733\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 28\n",
            "loss: 0.13009663070070332\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 29\n",
            "loss: 0.12889031279703667\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 30\n",
            "loss: 0.1256755541624694\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 31\n",
            "loss: 0.12160354791016414\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "2829 3315 4632\n",
            "test_answer_acc 0.6107512953367875 0.7156735751295337\n",
            "testing time 0h 5m 10s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 32\n",
            "loss: 0.11818314703374073\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 33\n",
            "loss: 0.11572061314665039\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 34\n",
            "loss: 0.11344971749289283\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 35\n",
            "loss: 0.1109314942154391\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 36\n",
            "loss: 0.11019478075463196\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 37\n",
            "loss: 0.10896338157612702\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 38\n",
            "loss: 0.10614325381044684\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 39\n",
            "loss: 0.10466263319911628\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 40\n",
            "loss: 0.10111412953200011\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 41\n",
            "loss: 0.08867794439710419\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "2879 3370 4632\n",
            "test_answer_acc 0.6215457685664939 0.7275474956822107\n",
            "testing time 0h 5m 8s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 42\n",
            "loss: 0.07876673417872396\n",
            "training time 0h 2m 38s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 43\n",
            "loss: 0.073680940861332\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 44\n",
            "loss: 0.0708804988270176\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 45\n",
            "loss: 0.06684073420434163\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 46\n",
            "loss: 0.06529559985060117\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 47\n",
            "loss: 0.06456208336969901\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 48\n",
            "loss: 0.060625603949201516\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 49\n",
            "loss: 0.06185066332590991\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 50\n",
            "loss: 0.05987361800567857\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 51\n",
            "loss: 0.06230400512958395\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "2859 3359 4632\n",
            "test_answer_acc 0.6172279792746114 0.7251727115716753\n",
            "testing time 0h 5m 20s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 52\n",
            "loss: 0.058700092316701494\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 53\n",
            "loss: 0.05693806641060731\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 54\n",
            "loss: 0.05592719588814111\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 55\n",
            "loss: 0.05651791176662363\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 56\n",
            "loss: 0.05454601903670821\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 57\n",
            "loss: 0.05603535344888424\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 58\n",
            "loss: 0.055503953158341605\n",
            "training time 0h 2m 39s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 59\n",
            "loss: 0.05428009259289709\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 60\n",
            "loss: 0.0546055823821446\n",
            "training time 0h 2m 46s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 61\n",
            "loss: 0.04951483300276872\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "2885 3390 4632\n",
            "test_answer_acc 0.6228411053540587 0.7318652849740933\n",
            "testing time 0h 5m 19s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 62\n",
            "loss: 0.04434020803149404\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 63\n",
            "loss: 0.04440515728603149\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 64\n",
            "loss: 0.0428132646877704\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 65\n",
            "loss: 0.04138089342620866\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 66\n",
            "loss: 0.039625739383286444\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 67\n",
            "loss: 0.03923146205729452\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 68\n",
            "loss: 0.03925031860326898\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 69\n",
            "loss: 0.03787813913077116\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 70\n",
            "loss: 0.0380736890345298\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 71\n",
            "loss: 0.038698968689503344\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "2870 3391 4632\n",
            "test_answer_acc 0.6196027633851469 0.7320811744386874\n",
            "testing time 0h 5m 9s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 72\n",
            "loss: 0.03761381833846199\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 73\n",
            "loss: 0.0354513296279414\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 74\n",
            "loss: 0.03735738403067507\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 75\n",
            "loss: 0.0362349521818346\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 76\n",
            "loss: 0.03622983527080766\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 1\n",
            "epoch: 77\n",
            "loss: 0.035071191981691736\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "2880 3395 4632\n",
            "test_answer_acc 0.6217616580310881 0.7329447322970639\n",
            "testing time 0h 5m 18s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 78\n",
            "loss: 0.03402223589605299\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "2889 3404 4632\n",
            "test_answer_acc 0.6237046632124352 0.7348877374784111\n",
            "testing time 0h 5m 14s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 79\n",
            "loss: 0.0347868955918941\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "2898 3405 4632\n",
            "test_answer_acc 0.6256476683937824 0.7351036269430051\n",
            "testing time 0h 5m 14s\n",
            "------------------------------------------------------\n",
            "fold: 1\n",
            "epoch: 80\n",
            "loss: 0.03463903007193886\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "2880 3398 4632\n",
            "test_answer_acc 0.6217616580310881 0.7335924006908463\n",
            "testing time 0h 5m 15s\n",
            "------------------------------------------------------\n",
            "Indexing words...\n",
            "keep_words 3687 / 10014 = 0.3682\n",
            "Indexed 3690 words in input language, 23 words in output\n",
            "Number of training data 18530\n",
            "Number of testind data 4632\n",
            "Vocabs Saved\n",
            "TIL: 3690\n",
            "PAD\n",
            "3690\n",
            "*\n",
            "23\n",
            "([2, 2, 3, 4, 5, 6, 7, 8, 9, 5, 10, 11, 12, 6, 13, 14, 1, 15, 16, 17, 18, 19, 20, 21, 16, 22, 23, 24, 25, 26, 27, 28, 16, 22, 1, 29, 25, 30, 31, 9, 32, 33, 15, 12], 44, [0, 1, 8, 5, 7], 5, ['2', '11'], [16, 34], [])\n",
            "Input size: 3690\n",
            " Input Embeddings Loaded\n",
            "Output Embeddings Loaded\n",
            "fold: 2\n",
            "epoch: 1\n",
            "loss: 1.5815118082638444\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "773 828 4632\n",
            "test_answer_acc 0.1668825561312608 0.17875647668393782\n",
            "testing time 0h 4m 41s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 2\n",
            "loss: 0.9992702525237511\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 3\n",
            "loss: 0.7628406573986185\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 4\n",
            "loss: 0.6451436807369364\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 5\n",
            "loss: 0.5687585039385434\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 6\n",
            "loss: 0.5149124505191014\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 7\n",
            "loss: 0.46657729498271283\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 8\n",
            "loss: 0.43239149944535615\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 9\n",
            "loss: 0.40490859253653166\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 10\n",
            "loss: 0.37575498141091446\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 11\n",
            "loss: 0.3544221086748715\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "2797 3183 4632\n",
            "test_answer_acc 0.6038428324697754 0.6871761658031088\n",
            "testing time 0h 5m 21s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 12\n",
            "loss: 0.3347450471129911\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 13\n",
            "loss: 0.3185469534890405\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 14\n",
            "loss: 0.3051876582976045\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 15\n",
            "loss: 0.29055615982104993\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 16\n",
            "loss: 0.28197974480431653\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 17\n",
            "loss: 0.2657264737219646\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 18\n",
            "loss: 0.2551043366563731\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 19\n",
            "loss: 0.24566704209508566\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 20\n",
            "loss: 0.24011515450888665\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 21\n",
            "loss: 0.19922131459260808\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "2927 3400 4632\n",
            "test_answer_acc 0.6319084628670121 0.7340241796200345\n",
            "testing time 0h 5m 21s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 22\n",
            "loss: 0.17035200164235872\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 23\n",
            "loss: 0.16139809669091784\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 24\n",
            "loss: 0.15430721157583696\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 25\n",
            "loss: 0.14899100378669541\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 26\n",
            "loss: 0.14290657762823433\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 27\n",
            "loss: 0.13695521118312046\n",
            "training time 0h 2m 47s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 28\n",
            "loss: 0.13367444770089512\n",
            "training time 0h 2m 46s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 29\n",
            "loss: 0.13221494383339225\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 30\n",
            "loss: 0.13012415247744527\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 31\n",
            "loss: 0.12473674565553665\n",
            "training time 0h 2m 46s\n",
            "--------------------------------\n",
            "2930 3400 4632\n",
            "test_answer_acc 0.6325561312607945 0.7340241796200345\n",
            "testing time 0h 5m 29s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 32\n",
            "loss: 0.12260505675241865\n",
            "training time 0h 2m 47s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 33\n",
            "loss: 0.12074684392789314\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 34\n",
            "loss: 0.11750434123236557\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 35\n",
            "loss: 0.11297879070043564\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 36\n",
            "loss: 0.11026417936744361\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 37\n",
            "loss: 0.10946709351806805\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 38\n",
            "loss: 0.10539534251237738\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 39\n",
            "loss: 0.10470152075948386\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 40\n",
            "loss: 0.10447553858674806\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 41\n",
            "loss: 0.08835225894019522\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "2955 3434 4632\n",
            "test_answer_acc 0.6379533678756477 0.7413644214162349\n",
            "testing time 0h 5m 18s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 42\n",
            "loss: 0.08090312956222172\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 43\n",
            "loss: 0.0759808004673185\n",
            "training time 0h 2m 46s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 44\n",
            "loss: 0.0744926927675461\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 45\n",
            "loss: 0.06962968206097339\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 46\n",
            "loss: 0.06769859725288276\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 47\n",
            "loss: 0.06562271333973983\n",
            "training time 0h 2m 46s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 48\n",
            "loss: 0.06561282464656337\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 49\n",
            "loss: 0.06370828511386083\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 50\n",
            "loss: 0.06118237734611692\n",
            "training time 0h 2m 46s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 51\n",
            "loss: 0.060129571873052366\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "2952 3440 4632\n",
            "test_answer_acc 0.6373056994818653 0.7426597582037997\n",
            "testing time 0h 5m 14s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 52\n",
            "loss: 0.06012262434794985\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 53\n",
            "loss: 0.05696289296550997\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 54\n",
            "loss: 0.05706006117678922\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 55\n",
            "loss: 0.05707463646500275\n",
            "training time 0h 2m 45s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 56\n",
            "loss: 0.0594880902947023\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 57\n",
            "loss: 0.057049412781308435\n",
            "training time 0h 2m 48s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 58\n",
            "loss: 0.057022966136192454\n",
            "training time 0h 2m 50s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 59\n",
            "loss: 0.05448454120035829\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 60\n",
            "loss: 0.05435460782770453\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 61\n",
            "loss: 0.04848912612631403\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "2969 3460 4632\n",
            "test_answer_acc 0.6409758203799655 0.7469775474956822\n",
            "testing time 0h 5m 15s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 62\n",
            "loss: 0.044114543330566636\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 63\n",
            "loss: 0.04469432415890283\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 64\n",
            "loss: 0.04230864623497272\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 65\n",
            "loss: 0.04214867480613034\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 66\n",
            "loss: 0.042756723901578066\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 67\n",
            "loss: 0.041622049438542336\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 68\n",
            "loss: 0.04118943877261261\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 69\n",
            "loss: 0.0402572913994563\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 70\n",
            "loss: 0.03829266071191122\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 71\n",
            "loss: 0.03721124177862858\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "2959 3444 4632\n",
            "test_answer_acc 0.6388169257340242 0.7435233160621761\n",
            "testing time 0h 5m 10s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 72\n",
            "loss: 0.03698387207656071\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 73\n",
            "loss: 0.03784695669751743\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 74\n",
            "loss: 0.03665182844189734\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 75\n",
            "loss: 0.0363521040192452\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 76\n",
            "loss: 0.03707245845219185\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 2\n",
            "epoch: 77\n",
            "loss: 0.03461681926173383\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "2959 3456 4632\n",
            "test_answer_acc 0.6388169257340242 0.7461139896373057\n",
            "testing time 0h 5m 9s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 78\n",
            "loss: 0.03439327601086477\n",
            "training time 0h 2m 41s\n",
            "--------------------------------\n",
            "2944 3442 4632\n",
            "test_answer_acc 0.6355785837651122 0.7430915371329879\n",
            "testing time 0h 5m 15s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 79\n",
            "loss: 0.036643697938014724\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "2951 3460 4632\n",
            "test_answer_acc 0.6370898100172712 0.7469775474956822\n",
            "testing time 0h 5m 17s\n",
            "------------------------------------------------------\n",
            "fold: 2\n",
            "epoch: 80\n",
            "loss: 0.036550416431293406\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "2945 3447 4632\n",
            "test_answer_acc 0.6357944732297064 0.7441709844559585\n",
            "testing time 0h 5m 15s\n",
            "------------------------------------------------------\n",
            "Indexing words...\n",
            "keep_words 3675 / 10000 = 0.3675\n",
            "Indexed 3678 words in input language, 23 words in output\n",
            "Number of training data 18530\n",
            "Number of testind data 4632\n",
            "Vocabs Saved\n",
            "TIL: 3678\n",
            "PAD\n",
            "3678\n",
            "*\n",
            "23\n",
            "([2, 2, 3, 4, 5, 6, 7, 8, 9, 5, 10, 11, 12, 6, 13, 14, 1, 15, 16, 17, 18, 19, 20, 21, 16, 22, 23, 24, 25, 26, 27, 28, 16, 22, 1, 29, 25, 30, 31, 9, 32, 33, 15, 12], 44, [0, 1, 8, 5, 7], 5, ['2', '11'], [16, 34], [])\n",
            "Input size: 3678\n",
            " Input Embeddings Loaded\n",
            "Output Embeddings Loaded\n",
            "fold: 3\n",
            "epoch: 1\n",
            "loss: 1.5710375983139564\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "796 882 4632\n",
            "test_answer_acc 0.17184801381692574 0.19041450777202074\n",
            "testing time 0h 4m 59s\n",
            "------------------------------------------------------\n",
            "fold: 3\n",
            "epoch: 2\n",
            "loss: 0.9744334286656873\n",
            "training time 0h 2m 42s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 3\n",
            "loss: 0.7448901649179129\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 4\n",
            "loss: 0.6353636180532389\n",
            "training time 0h 2m 40s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 5\n",
            "loss: 0.5613866713540308\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 6\n",
            "loss: 0.5033738150678831\n",
            "training time 0h 2m 43s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 7\n",
            "loss: 0.4596036210142333\n",
            "training time 0h 2m 44s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 8\n",
            "loss: 0.4246348958590935\n",
            "training time 0h 2m 46s\n",
            "--------------------------------\n",
            "fold: 3\n",
            "epoch: 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcjKe0K34yoH"
      },
      "source": [
        "**Trying out modification 2 to incorporate transformer encoder replacing the GRU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE7oSakoOWWF",
        "outputId": "44ecd670-3497-4e9e-f11b-3fac9340abaf"
      },
      "source": [
        "#Transformer encoder\n",
        "!python3 run_seq2tree.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Transfer numbers...\n",
            "(['镇海', '雅乐', '学校', '二年级', '的', '小朋友', '到', '一条', '小路', '的', '一边', '植树', '．', '小朋友', '们', '每隔', 'NUM', '米', '种', '一棵树', '（', '马路', '两头', '都', '种', '了', '树', '）', '，', '最后', '发现', '一共', '种', '了', 'NUM', '棵', '，', '这', '条', '小路', '长', '多少', '米', '．'], ['(', 'N1', '-', '1', ')', '*', 'N0'], ['2', '11'], [16, 34])\n",
            "Indexing words...\n",
            "keep_words 3671 / 9971 = 0.3682\n",
            "Indexed 3674 words in input language, 23 words in output\n",
            "Number of training data 18530\n",
            "Number of testind data 4632\n",
            "Vocabs Saved\n",
            "TIL: 3674\n",
            "PAD\n",
            "3674\n",
            "*\n",
            "23\n",
            "([3, 4, 5, 1, 6, 7, 8, 9, 10, 11, 12, 13, 1, 14, 10, 15, 1, 16, 17, 2, 10, 18, 19, 20, 21, 16, 22], 27, [0, 9, 8], 3, ['5', '0.8', '150'], [3, 12, 16], [])\n",
            "Input size: 3674\n",
            " Input Embeddings Loaded\n",
            "Output Embeddings Loaded\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "fold: 1\n",
            "epoch: 1\n",
            "Pade torch.Size([82, 64, 768])\n",
            "Traceback (most recent call last):\n",
            "  File \"run_seq2tree.py\", line 112, in <module>\n",
            "    encoder_optimizer, predict_optimizer, generate_optimizer, merge_optimizer, output_lang, num_pos_batches[idx])\n",
            "  File \"/content/gdrive/MyDrive/math_seq2tree/src/train_and_evaluate.py\", line 688, in train_tree\n",
            "    encoder_outputs, problem_output = encoder(input_var, input_length)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/gdrive/MyDrive/math_seq2tree/src/models.py\", line 219, in forward\n",
            "    problem_output = pade_outputs[-1, :, :self.hidden_size] + pade_outputs[0, :, self.hidden_size:]\n",
            "RuntimeError: The size of tensor a (64) must match the size of tensor b (704) at non-singleton dimension 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVAfMBGWPFhw",
        "outputId": "b7eda510-aa80-40f0-ae68-e9f57b35c312"
      },
      "source": [
        "import torch\n",
        "#multihead_attn = torch.nn.MultiheadAttention(embed_dim, num_heads)\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0.1.post2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c543E65BEiLF"
      },
      "source": [
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu-tfTEQC5-8",
        "outputId": "0af427f0-8cbb-41fc-8ad4-1da4b091415e"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 4.2MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 17.4MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 52.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 53.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=f2c5d66bb20e841c72c4922e54624ffe889fd1973872d071407c459d2c558e7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JTyeJO0CACh",
        "outputId": "b3a02f3b-5f7b-49d0-84c6-e30126bb8a60"
      },
      "source": [
        "import json\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "vocab = {}\n",
        "with open(\"vocab_dict.json\",'r') as f:\n",
        "  vocab = json.load(f)\n",
        "\n",
        "words_list = list(vocab.keys())\n",
        "\n",
        "print(words_list[0])\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
        "\n",
        "BertModel = BertModel.from_pretrained('bert-base-chinese', output_hidden_states = True).to(device)\n",
        "BertModel.eval()\n",
        "\n",
        "cap = words_list[0]\n",
        "                \n",
        "tokenized_cap = tokenizer.tokenize(cap)                \n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_cap)\n",
        "tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  encoded_layers = BertModel(tokens_tensor)\n",
        "\n",
        "bert_embedding = encoded_layers[2][11].squeeze(0)\n",
        "\n",
        "\n",
        "\n",
        "#tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
        "#marked_text = \"[CLS] \" + words_list[0] + \" [SEP]\"\n",
        "#tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "#print(tokenized_text)\n",
        "\n",
        "split_cap = cap.split()\n",
        "tokens_embedding = []\n",
        "j = 0\n",
        "\n",
        "for full_token in split_cap:\n",
        "    curr_token = ''\n",
        "    x = 0\n",
        "    for i,_ in enumerate(tokenized_cap[1:]): # disregard CLS\n",
        "        token = tokenized_cap[i+j]\n",
        "        print(token)\n",
        "        piece_embedding = bert_embedding[i+j]\n",
        "        \n",
        "        # full token\n",
        "        if token == full_token and curr_token == '' :\n",
        "            tokens_embedding.append(piece_embedding)\n",
        "            j += 1\n",
        "            break\n",
        "        else: # partial token\n",
        "            x += 1\n",
        "            \n",
        "            if curr_token == '':\n",
        "                tokens_embedding.append(piece_embedding)\n",
        "                curr_token += token.replace('#', '')\n",
        "            else:\n",
        "                tokens_embedding[-1] = torch.add(tokens_embedding[-1], piece_embedding)\n",
        "                curr_token += token.replace('#', '')\n",
        "                \n",
        "                if curr_token == full_token: # end of partial\n",
        "                    j += x\n",
        "                    break                            \n",
        "\n",
        "cap_embedding = torch.stack(tokens_embedding)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "时代\n",
            "时\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L0FCX7MZS4F-",
        "outputId": "6413ad9d-3977-4c01-a906-f5b93184f0de"
      },
      "source": [
        "import json\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "import torch\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "\n",
        "vocab = {}\n",
        "emb_dict = {}\n",
        "with open(\"vocab_dict.json\",'r') as f:\n",
        "  vocab = json.load(f)\n",
        "\n",
        "words_list = list(vocab.keys())\n",
        "\n",
        "print(words_list[0])\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
        "\n",
        "BertModel = BertModel.from_pretrained('bert-base-chinese').to(device)\n",
        "BertModel.eval()\n",
        "\n",
        "print(len(words_list))\n",
        "for idx in range(len(words_list)):\n",
        "  print(idx)\n",
        "  cap = words_list[idx]\n",
        "                  \n",
        "  if cap != '\\u3000' and cap != '':\n",
        "    tokenized_cap = tokenizer.tokenize(cap)                \n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_cap)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      encoded_layers, _ = BertModel(tokens_tensor)\n",
        "\n",
        "    bert_embedding = encoded_layers[11].squeeze(0)\n",
        "\n",
        "    dim = bert_embedding.shape[0]\n",
        "\n",
        "    if dim > 1:\n",
        "      x = bert_embedding[0]\n",
        "      x = x.reshape(1,768)\n",
        "      for i in range(1,dim):\n",
        "        temp_emb = bert_embedding[i]\n",
        "        temp_emb = temp_emb.reshape(1,768)\n",
        "        x = torch.add(x,temp_emb)\n",
        "    else:\n",
        "      x = bert_embedding[0]\n",
        "      x = x.reshape(1,768)\n",
        "\n",
        "    x = x.to(\"cpu\")\n",
        "    x_np = x.data.numpy()\n",
        "  else:\n",
        "    x_np = np.random.randn(1, 768)\n",
        "\n",
        "  emb_dict[words_list[idx]] = x_np\n",
        "\n",
        "with open('embeddings_dict.pickle', 'wb') as handle:\n",
        "    pickle.dump(emb_dict, handle)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PAD\n",
            "3690\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "1000\n",
            "1001\n",
            "1002\n",
            "1003\n",
            "1004\n",
            "1005\n",
            "1006\n",
            "1007\n",
            "1008\n",
            "1009\n",
            "1010\n",
            "1011\n",
            "1012\n",
            "1013\n",
            "1014\n",
            "1015\n",
            "1016\n",
            "1017\n",
            "1018\n",
            "1019\n",
            "1020\n",
            "1021\n",
            "1022\n",
            "1023\n",
            "1024\n",
            "1025\n",
            "1026\n",
            "1027\n",
            "1028\n",
            "1029\n",
            "1030\n",
            "1031\n",
            "1032\n",
            "1033\n",
            "1034\n",
            "1035\n",
            "1036\n",
            "1037\n",
            "1038\n",
            "1039\n",
            "1040\n",
            "1041\n",
            "1042\n",
            "1043\n",
            "1044\n",
            "1045\n",
            "1046\n",
            "1047\n",
            "1048\n",
            "1049\n",
            "1050\n",
            "1051\n",
            "1052\n",
            "1053\n",
            "1054\n",
            "1055\n",
            "1056\n",
            "1057\n",
            "1058\n",
            "1059\n",
            "1060\n",
            "1061\n",
            "1062\n",
            "1063\n",
            "1064\n",
            "1065\n",
            "1066\n",
            "1067\n",
            "1068\n",
            "1069\n",
            "1070\n",
            "1071\n",
            "1072\n",
            "1073\n",
            "1074\n",
            "1075\n",
            "1076\n",
            "1077\n",
            "1078\n",
            "1079\n",
            "1080\n",
            "1081\n",
            "1082\n",
            "1083\n",
            "1084\n",
            "1085\n",
            "1086\n",
            "1087\n",
            "1088\n",
            "1089\n",
            "1090\n",
            "1091\n",
            "1092\n",
            "1093\n",
            "1094\n",
            "1095\n",
            "1096\n",
            "1097\n",
            "1098\n",
            "1099\n",
            "1100\n",
            "1101\n",
            "1102\n",
            "1103\n",
            "1104\n",
            "1105\n",
            "1106\n",
            "1107\n",
            "1108\n",
            "1109\n",
            "1110\n",
            "1111\n",
            "1112\n",
            "1113\n",
            "1114\n",
            "1115\n",
            "1116\n",
            "1117\n",
            "1118\n",
            "1119\n",
            "1120\n",
            "1121\n",
            "1122\n",
            "1123\n",
            "1124\n",
            "1125\n",
            "1126\n",
            "1127\n",
            "1128\n",
            "1129\n",
            "1130\n",
            "1131\n",
            "1132\n",
            "1133\n",
            "1134\n",
            "1135\n",
            "1136\n",
            "1137\n",
            "1138\n",
            "1139\n",
            "1140\n",
            "1141\n",
            "1142\n",
            "1143\n",
            "1144\n",
            "1145\n",
            "1146\n",
            "1147\n",
            "1148\n",
            "1149\n",
            "1150\n",
            "1151\n",
            "1152\n",
            "1153\n",
            "1154\n",
            "1155\n",
            "1156\n",
            "1157\n",
            "1158\n",
            "1159\n",
            "1160\n",
            "1161\n",
            "1162\n",
            "1163\n",
            "1164\n",
            "1165\n",
            "1166\n",
            "1167\n",
            "1168\n",
            "1169\n",
            "1170\n",
            "1171\n",
            "1172\n",
            "1173\n",
            "1174\n",
            "1175\n",
            "1176\n",
            "1177\n",
            "1178\n",
            "1179\n",
            "1180\n",
            "1181\n",
            "1182\n",
            "1183\n",
            "1184\n",
            "1185\n",
            "1186\n",
            "1187\n",
            "1188\n",
            "1189\n",
            "1190\n",
            "1191\n",
            "1192\n",
            "1193\n",
            "1194\n",
            "1195\n",
            "1196\n",
            "1197\n",
            "1198\n",
            "1199\n",
            "1200\n",
            "1201\n",
            "1202\n",
            "1203\n",
            "1204\n",
            "1205\n",
            "1206\n",
            "1207\n",
            "1208\n",
            "1209\n",
            "1210\n",
            "1211\n",
            "1212\n",
            "1213\n",
            "1214\n",
            "1215\n",
            "1216\n",
            "1217\n",
            "1218\n",
            "1219\n",
            "1220\n",
            "1221\n",
            "1222\n",
            "1223\n",
            "1224\n",
            "1225\n",
            "1226\n",
            "1227\n",
            "1228\n",
            "1229\n",
            "1230\n",
            "1231\n",
            "1232\n",
            "1233\n",
            "1234\n",
            "1235\n",
            "1236\n",
            "1237\n",
            "1238\n",
            "1239\n",
            "1240\n",
            "1241\n",
            "1242\n",
            "1243\n",
            "1244\n",
            "1245\n",
            "1246\n",
            "1247\n",
            "1248\n",
            "1249\n",
            "1250\n",
            "1251\n",
            "1252\n",
            "1253\n",
            "1254\n",
            "1255\n",
            "1256\n",
            "1257\n",
            "1258\n",
            "1259\n",
            "1260\n",
            "1261\n",
            "1262\n",
            "1263\n",
            "1264\n",
            "1265\n",
            "1266\n",
            "1267\n",
            "1268\n",
            "1269\n",
            "1270\n",
            "1271\n",
            "1272\n",
            "1273\n",
            "1274\n",
            "1275\n",
            "1276\n",
            "1277\n",
            "1278\n",
            "1279\n",
            "1280\n",
            "1281\n",
            "1282\n",
            "1283\n",
            "1284\n",
            "1285\n",
            "1286\n",
            "1287\n",
            "1288\n",
            "1289\n",
            "1290\n",
            "1291\n",
            "1292\n",
            "1293\n",
            "1294\n",
            "1295\n",
            "1296\n",
            "1297\n",
            "1298\n",
            "1299\n",
            "1300\n",
            "1301\n",
            "1302\n",
            "1303\n",
            "1304\n",
            "1305\n",
            "1306\n",
            "1307\n",
            "1308\n",
            "1309\n",
            "1310\n",
            "1311\n",
            "1312\n",
            "1313\n",
            "1314\n",
            "1315\n",
            "1316\n",
            "1317\n",
            "1318\n",
            "1319\n",
            "1320\n",
            "1321\n",
            "1322\n",
            "1323\n",
            "1324\n",
            "1325\n",
            "1326\n",
            "1327\n",
            "1328\n",
            "1329\n",
            "1330\n",
            "1331\n",
            "1332\n",
            "1333\n",
            "1334\n",
            "1335\n",
            "1336\n",
            "1337\n",
            "1338\n",
            "1339\n",
            "1340\n",
            "1341\n",
            "1342\n",
            "1343\n",
            "1344\n",
            "1345\n",
            "1346\n",
            "1347\n",
            "1348\n",
            "1349\n",
            "1350\n",
            "1351\n",
            "1352\n",
            "1353\n",
            "1354\n",
            "1355\n",
            "1356\n",
            "1357\n",
            "1358\n",
            "1359\n",
            "1360\n",
            "1361\n",
            "1362\n",
            "1363\n",
            "1364\n",
            "1365\n",
            "1366\n",
            "1367\n",
            "1368\n",
            "1369\n",
            "1370\n",
            "1371\n",
            "1372\n",
            "1373\n",
            "1374\n",
            "1375\n",
            "1376\n",
            "1377\n",
            "1378\n",
            "1379\n",
            "1380\n",
            "1381\n",
            "1382\n",
            "1383\n",
            "1384\n",
            "1385\n",
            "1386\n",
            "1387\n",
            "1388\n",
            "1389\n",
            "1390\n",
            "1391\n",
            "1392\n",
            "1393\n",
            "1394\n",
            "1395\n",
            "1396\n",
            "1397\n",
            "1398\n",
            "1399\n",
            "1400\n",
            "1401\n",
            "1402\n",
            "1403\n",
            "1404\n",
            "1405\n",
            "1406\n",
            "1407\n",
            "1408\n",
            "1409\n",
            "1410\n",
            "1411\n",
            "1412\n",
            "1413\n",
            "1414\n",
            "1415\n",
            "1416\n",
            "1417\n",
            "1418\n",
            "1419\n",
            "1420\n",
            "1421\n",
            "1422\n",
            "1423\n",
            "1424\n",
            "1425\n",
            "1426\n",
            "1427\n",
            "1428\n",
            "1429\n",
            "1430\n",
            "1431\n",
            "1432\n",
            "1433\n",
            "1434\n",
            "1435\n",
            "1436\n",
            "1437\n",
            "1438\n",
            "1439\n",
            "1440\n",
            "1441\n",
            "1442\n",
            "1443\n",
            "1444\n",
            "1445\n",
            "1446\n",
            "1447\n",
            "1448\n",
            "1449\n",
            "1450\n",
            "1451\n",
            "1452\n",
            "1453\n",
            "1454\n",
            "1455\n",
            "1456\n",
            "1457\n",
            "1458\n",
            "1459\n",
            "1460\n",
            "1461\n",
            "1462\n",
            "1463\n",
            "1464\n",
            "1465\n",
            "1466\n",
            "1467\n",
            "1468\n",
            "1469\n",
            "1470\n",
            "1471\n",
            "1472\n",
            "1473\n",
            "1474\n",
            "1475\n",
            "1476\n",
            "1477\n",
            "1478\n",
            "1479\n",
            "1480\n",
            "1481\n",
            "1482\n",
            "1483\n",
            "1484\n",
            "1485\n",
            "1486\n",
            "1487\n",
            "1488\n",
            "1489\n",
            "1490\n",
            "1491\n",
            "1492\n",
            "1493\n",
            "1494\n",
            "1495\n",
            "1496\n",
            "1497\n",
            "1498\n",
            "1499\n",
            "1500\n",
            "1501\n",
            "1502\n",
            "1503\n",
            "1504\n",
            "1505\n",
            "1506\n",
            "1507\n",
            "1508\n",
            "1509\n",
            "1510\n",
            "1511\n",
            "1512\n",
            "1513\n",
            "1514\n",
            "1515\n",
            "1516\n",
            "1517\n",
            "1518\n",
            "1519\n",
            "1520\n",
            "1521\n",
            "1522\n",
            "1523\n",
            "1524\n",
            "1525\n",
            "1526\n",
            "1527\n",
            "1528\n",
            "1529\n",
            "1530\n",
            "1531\n",
            "1532\n",
            "1533\n",
            "1534\n",
            "1535\n",
            "1536\n",
            "1537\n",
            "1538\n",
            "1539\n",
            "1540\n",
            "1541\n",
            "1542\n",
            "1543\n",
            "1544\n",
            "1545\n",
            "1546\n",
            "1547\n",
            "1548\n",
            "1549\n",
            "1550\n",
            "1551\n",
            "1552\n",
            "1553\n",
            "1554\n",
            "1555\n",
            "1556\n",
            "1557\n",
            "1558\n",
            "1559\n",
            "1560\n",
            "1561\n",
            "1562\n",
            "1563\n",
            "1564\n",
            "1565\n",
            "1566\n",
            "1567\n",
            "1568\n",
            "1569\n",
            "1570\n",
            "1571\n",
            "1572\n",
            "1573\n",
            "1574\n",
            "1575\n",
            "1576\n",
            "1577\n",
            "1578\n",
            "1579\n",
            "1580\n",
            "1581\n",
            "1582\n",
            "1583\n",
            "1584\n",
            "1585\n",
            "1586\n",
            "1587\n",
            "1588\n",
            "1589\n",
            "1590\n",
            "1591\n",
            "1592\n",
            "1593\n",
            "1594\n",
            "1595\n",
            "1596\n",
            "1597\n",
            "1598\n",
            "1599\n",
            "1600\n",
            "1601\n",
            "1602\n",
            "1603\n",
            "1604\n",
            "1605\n",
            "1606\n",
            "1607\n",
            "1608\n",
            "1609\n",
            "1610\n",
            "1611\n",
            "1612\n",
            "1613\n",
            "1614\n",
            "1615\n",
            "1616\n",
            "1617\n",
            "1618\n",
            "1619\n",
            "1620\n",
            "1621\n",
            "1622\n",
            "1623\n",
            "1624\n",
            "1625\n",
            "1626\n",
            "1627\n",
            "1628\n",
            "1629\n",
            "1630\n",
            "1631\n",
            "1632\n",
            "1633\n",
            "1634\n",
            "1635\n",
            "1636\n",
            "1637\n",
            "1638\n",
            "1639\n",
            "1640\n",
            "1641\n",
            "1642\n",
            "1643\n",
            "1644\n",
            "1645\n",
            "1646\n",
            "1647\n",
            "1648\n",
            "1649\n",
            "1650\n",
            "1651\n",
            "1652\n",
            "1653\n",
            "1654\n",
            "1655\n",
            "1656\n",
            "1657\n",
            "1658\n",
            "1659\n",
            "1660\n",
            "1661\n",
            "1662\n",
            "1663\n",
            "1664\n",
            "1665\n",
            "1666\n",
            "1667\n",
            "1668\n",
            "1669\n",
            "1670\n",
            "1671\n",
            "1672\n",
            "1673\n",
            "1674\n",
            "1675\n",
            "1676\n",
            "1677\n",
            "1678\n",
            "1679\n",
            "1680\n",
            "1681\n",
            "1682\n",
            "1683\n",
            "1684\n",
            "1685\n",
            "1686\n",
            "1687\n",
            "1688\n",
            "1689\n",
            "1690\n",
            "1691\n",
            "1692\n",
            "1693\n",
            "1694\n",
            "1695\n",
            "1696\n",
            "1697\n",
            "1698\n",
            "1699\n",
            "1700\n",
            "1701\n",
            "1702\n",
            "1703\n",
            "1704\n",
            "1705\n",
            "1706\n",
            "1707\n",
            "1708\n",
            "1709\n",
            "1710\n",
            "1711\n",
            "1712\n",
            "1713\n",
            "1714\n",
            "1715\n",
            "1716\n",
            "1717\n",
            "1718\n",
            "1719\n",
            "1720\n",
            "1721\n",
            "1722\n",
            "1723\n",
            "1724\n",
            "1725\n",
            "1726\n",
            "1727\n",
            "1728\n",
            "1729\n",
            "1730\n",
            "1731\n",
            "1732\n",
            "1733\n",
            "1734\n",
            "1735\n",
            "1736\n",
            "1737\n",
            "1738\n",
            "1739\n",
            "1740\n",
            "1741\n",
            "1742\n",
            "1743\n",
            "1744\n",
            "1745\n",
            "1746\n",
            "1747\n",
            "1748\n",
            "1749\n",
            "1750\n",
            "1751\n",
            "1752\n",
            "1753\n",
            "1754\n",
            "1755\n",
            "1756\n",
            "1757\n",
            "1758\n",
            "1759\n",
            "1760\n",
            "1761\n",
            "1762\n",
            "1763\n",
            "1764\n",
            "1765\n",
            "1766\n",
            "1767\n",
            "1768\n",
            "1769\n",
            "1770\n",
            "1771\n",
            "1772\n",
            "1773\n",
            "1774\n",
            "1775\n",
            "1776\n",
            "1777\n",
            "1778\n",
            "1779\n",
            "1780\n",
            "1781\n",
            "1782\n",
            "1783\n",
            "1784\n",
            "1785\n",
            "1786\n",
            "1787\n",
            "1788\n",
            "1789\n",
            "1790\n",
            "1791\n",
            "1792\n",
            "1793\n",
            "1794\n",
            "1795\n",
            "1796\n",
            "1797\n",
            "1798\n",
            "1799\n",
            "1800\n",
            "1801\n",
            "1802\n",
            "1803\n",
            "1804\n",
            "1805\n",
            "1806\n",
            "1807\n",
            "1808\n",
            "1809\n",
            "1810\n",
            "1811\n",
            "1812\n",
            "1813\n",
            "1814\n",
            "1815\n",
            "1816\n",
            "1817\n",
            "1818\n",
            "1819\n",
            "1820\n",
            "1821\n",
            "1822\n",
            "1823\n",
            "1824\n",
            "1825\n",
            "1826\n",
            "1827\n",
            "1828\n",
            "1829\n",
            "1830\n",
            "1831\n",
            "1832\n",
            "1833\n",
            "1834\n",
            "1835\n",
            "1836\n",
            "1837\n",
            "1838\n",
            "1839\n",
            "1840\n",
            "1841\n",
            "1842\n",
            "1843\n",
            "1844\n",
            "1845\n",
            "1846\n",
            "1847\n",
            "1848\n",
            "1849\n",
            "1850\n",
            "1851\n",
            "1852\n",
            "1853\n",
            "1854\n",
            "1855\n",
            "1856\n",
            "1857\n",
            "1858\n",
            "1859\n",
            "1860\n",
            "1861\n",
            "1862\n",
            "1863\n",
            "1864\n",
            "1865\n",
            "1866\n",
            "1867\n",
            "1868\n",
            "1869\n",
            "1870\n",
            "1871\n",
            "1872\n",
            "1873\n",
            "1874\n",
            "1875\n",
            "1876\n",
            "1877\n",
            "1878\n",
            "1879\n",
            "1880\n",
            "1881\n",
            "1882\n",
            "1883\n",
            "1884\n",
            "1885\n",
            "1886\n",
            "1887\n",
            "1888\n",
            "1889\n",
            "1890\n",
            "1891\n",
            "1892\n",
            "1893\n",
            "1894\n",
            "1895\n",
            "1896\n",
            "1897\n",
            "1898\n",
            "1899\n",
            "1900\n",
            "1901\n",
            "1902\n",
            "1903\n",
            "1904\n",
            "1905\n",
            "1906\n",
            "1907\n",
            "1908\n",
            "1909\n",
            "1910\n",
            "1911\n",
            "1912\n",
            "1913\n",
            "1914\n",
            "1915\n",
            "1916\n",
            "1917\n",
            "1918\n",
            "1919\n",
            "1920\n",
            "1921\n",
            "1922\n",
            "1923\n",
            "1924\n",
            "1925\n",
            "1926\n",
            "1927\n",
            "1928\n",
            "1929\n",
            "1930\n",
            "1931\n",
            "1932\n",
            "1933\n",
            "1934\n",
            "1935\n",
            "1936\n",
            "1937\n",
            "1938\n",
            "1939\n",
            "1940\n",
            "1941\n",
            "1942\n",
            "1943\n",
            "1944\n",
            "1945\n",
            "1946\n",
            "1947\n",
            "1948\n",
            "1949\n",
            "1950\n",
            "1951\n",
            "1952\n",
            "1953\n",
            "1954\n",
            "1955\n",
            "1956\n",
            "1957\n",
            "1958\n",
            "1959\n",
            "1960\n",
            "1961\n",
            "1962\n",
            "1963\n",
            "1964\n",
            "1965\n",
            "1966\n",
            "1967\n",
            "1968\n",
            "1969\n",
            "1970\n",
            "1971\n",
            "1972\n",
            "1973\n",
            "1974\n",
            "1975\n",
            "1976\n",
            "1977\n",
            "1978\n",
            "1979\n",
            "1980\n",
            "1981\n",
            "1982\n",
            "1983\n",
            "1984\n",
            "1985\n",
            "1986\n",
            "1987\n",
            "1988\n",
            "1989\n",
            "1990\n",
            "1991\n",
            "1992\n",
            "1993\n",
            "1994\n",
            "1995\n",
            "1996\n",
            "1997\n",
            "1998\n",
            "1999\n",
            "2000\n",
            "2001\n",
            "2002\n",
            "2003\n",
            "2004\n",
            "2005\n",
            "2006\n",
            "2007\n",
            "2008\n",
            "2009\n",
            "2010\n",
            "2011\n",
            "2012\n",
            "2013\n",
            "2014\n",
            "2015\n",
            "2016\n",
            "2017\n",
            "2018\n",
            "2019\n",
            "2020\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024\n",
            "2025\n",
            "2026\n",
            "2027\n",
            "2028\n",
            "2029\n",
            "2030\n",
            "2031\n",
            "2032\n",
            "2033\n",
            "2034\n",
            "2035\n",
            "2036\n",
            "2037\n",
            "2038\n",
            "2039\n",
            "2040\n",
            "2041\n",
            "2042\n",
            "2043\n",
            "2044\n",
            "2045\n",
            "2046\n",
            "2047\n",
            "2048\n",
            "2049\n",
            "2050\n",
            "2051\n",
            "2052\n",
            "2053\n",
            "2054\n",
            "2055\n",
            "2056\n",
            "2057\n",
            "2058\n",
            "2059\n",
            "2060\n",
            "2061\n",
            "2062\n",
            "2063\n",
            "2064\n",
            "2065\n",
            "2066\n",
            "2067\n",
            "2068\n",
            "2069\n",
            "2070\n",
            "2071\n",
            "2072\n",
            "2073\n",
            "2074\n",
            "2075\n",
            "2076\n",
            "2077\n",
            "2078\n",
            "2079\n",
            "2080\n",
            "2081\n",
            "2082\n",
            "2083\n",
            "2084\n",
            "2085\n",
            "2086\n",
            "2087\n",
            "2088\n",
            "2089\n",
            "2090\n",
            "2091\n",
            "2092\n",
            "2093\n",
            "2094\n",
            "2095\n",
            "2096\n",
            "2097\n",
            "2098\n",
            "2099\n",
            "2100\n",
            "2101\n",
            "2102\n",
            "2103\n",
            "2104\n",
            "2105\n",
            "2106\n",
            "2107\n",
            "2108\n",
            "2109\n",
            "2110\n",
            "2111\n",
            "2112\n",
            "2113\n",
            "2114\n",
            "2115\n",
            "2116\n",
            "2117\n",
            "2118\n",
            "2119\n",
            "2120\n",
            "2121\n",
            "2122\n",
            "2123\n",
            "2124\n",
            "2125\n",
            "2126\n",
            "2127\n",
            "2128\n",
            "2129\n",
            "2130\n",
            "2131\n",
            "2132\n",
            "2133\n",
            "2134\n",
            "2135\n",
            "2136\n",
            "2137\n",
            "2138\n",
            "2139\n",
            "2140\n",
            "2141\n",
            "2142\n",
            "2143\n",
            "2144\n",
            "2145\n",
            "2146\n",
            "2147\n",
            "2148\n",
            "2149\n",
            "2150\n",
            "2151\n",
            "2152\n",
            "2153\n",
            "2154\n",
            "2155\n",
            "2156\n",
            "2157\n",
            "2158\n",
            "2159\n",
            "2160\n",
            "2161\n",
            "2162\n",
            "2163\n",
            "2164\n",
            "2165\n",
            "2166\n",
            "2167\n",
            "2168\n",
            "2169\n",
            "2170\n",
            "2171\n",
            "2172\n",
            "2173\n",
            "2174\n",
            "2175\n",
            "2176\n",
            "2177\n",
            "2178\n",
            "2179\n",
            "2180\n",
            "2181\n",
            "2182\n",
            "2183\n",
            "2184\n",
            "2185\n",
            "2186\n",
            "2187\n",
            "2188\n",
            "2189\n",
            "2190\n",
            "2191\n",
            "2192\n",
            "2193\n",
            "2194\n",
            "2195\n",
            "2196\n",
            "2197\n",
            "2198\n",
            "2199\n",
            "2200\n",
            "2201\n",
            "2202\n",
            "2203\n",
            "2204\n",
            "2205\n",
            "2206\n",
            "2207\n",
            "2208\n",
            "2209\n",
            "2210\n",
            "2211\n",
            "2212\n",
            "2213\n",
            "2214\n",
            "2215\n",
            "2216\n",
            "2217\n",
            "2218\n",
            "2219\n",
            "2220\n",
            "2221\n",
            "2222\n",
            "2223\n",
            "2224\n",
            "2225\n",
            "2226\n",
            "2227\n",
            "2228\n",
            "2229\n",
            "2230\n",
            "2231\n",
            "2232\n",
            "2233\n",
            "2234\n",
            "2235\n",
            "2236\n",
            "2237\n",
            "2238\n",
            "2239\n",
            "2240\n",
            "2241\n",
            "2242\n",
            "2243\n",
            "2244\n",
            "2245\n",
            "2246\n",
            "2247\n",
            "2248\n",
            "2249\n",
            "2250\n",
            "2251\n",
            "2252\n",
            "2253\n",
            "2254\n",
            "2255\n",
            "2256\n",
            "2257\n",
            "2258\n",
            "2259\n",
            "2260\n",
            "2261\n",
            "2262\n",
            "2263\n",
            "2264\n",
            "2265\n",
            "2266\n",
            "2267\n",
            "2268\n",
            "2269\n",
            "2270\n",
            "2271\n",
            "2272\n",
            "2273\n",
            "2274\n",
            "2275\n",
            "2276\n",
            "2277\n",
            "2278\n",
            "2279\n",
            "2280\n",
            "2281\n",
            "2282\n",
            "2283\n",
            "2284\n",
            "2285\n",
            "2286\n",
            "2287\n",
            "2288\n",
            "2289\n",
            "2290\n",
            "2291\n",
            "2292\n",
            "2293\n",
            "2294\n",
            "2295\n",
            "2296\n",
            "2297\n",
            "2298\n",
            "2299\n",
            "2300\n",
            "2301\n",
            "2302\n",
            "2303\n",
            "2304\n",
            "2305\n",
            "2306\n",
            "2307\n",
            "2308\n",
            "2309\n",
            "2310\n",
            "2311\n",
            "2312\n",
            "2313\n",
            "2314\n",
            "2315\n",
            "2316\n",
            "2317\n",
            "2318\n",
            "2319\n",
            "2320\n",
            "2321\n",
            "2322\n",
            "2323\n",
            "2324\n",
            "2325\n",
            "2326\n",
            "2327\n",
            "2328\n",
            "2329\n",
            "2330\n",
            "2331\n",
            "2332\n",
            "2333\n",
            "2334\n",
            "2335\n",
            "2336\n",
            "2337\n",
            "2338\n",
            "2339\n",
            "2340\n",
            "2341\n",
            "2342\n",
            "2343\n",
            "2344\n",
            "2345\n",
            "2346\n",
            "2347\n",
            "2348\n",
            "2349\n",
            "2350\n",
            "2351\n",
            "2352\n",
            "2353\n",
            "2354\n",
            "2355\n",
            "2356\n",
            "2357\n",
            "2358\n",
            "2359\n",
            "2360\n",
            "2361\n",
            "2362\n",
            "2363\n",
            "2364\n",
            "2365\n",
            "2366\n",
            "2367\n",
            "2368\n",
            "2369\n",
            "2370\n",
            "2371\n",
            "2372\n",
            "2373\n",
            "2374\n",
            "2375\n",
            "2376\n",
            "2377\n",
            "2378\n",
            "2379\n",
            "2380\n",
            "2381\n",
            "2382\n",
            "2383\n",
            "2384\n",
            "2385\n",
            "2386\n",
            "2387\n",
            "2388\n",
            "2389\n",
            "2390\n",
            "2391\n",
            "2392\n",
            "2393\n",
            "2394\n",
            "2395\n",
            "2396\n",
            "2397\n",
            "2398\n",
            "2399\n",
            "2400\n",
            "2401\n",
            "2402\n",
            "2403\n",
            "2404\n",
            "2405\n",
            "2406\n",
            "2407\n",
            "2408\n",
            "2409\n",
            "2410\n",
            "2411\n",
            "2412\n",
            "2413\n",
            "2414\n",
            "2415\n",
            "2416\n",
            "2417\n",
            "2418\n",
            "2419\n",
            "2420\n",
            "2421\n",
            "2422\n",
            "2423\n",
            "2424\n",
            "2425\n",
            "2426\n",
            "2427\n",
            "2428\n",
            "2429\n",
            "2430\n",
            "2431\n",
            "2432\n",
            "2433\n",
            "2434\n",
            "2435\n",
            "2436\n",
            "2437\n",
            "2438\n",
            "2439\n",
            "2440\n",
            "2441\n",
            "2442\n",
            "2443\n",
            "2444\n",
            "2445\n",
            "2446\n",
            "2447\n",
            "2448\n",
            "2449\n",
            "2450\n",
            "2451\n",
            "2452\n",
            "2453\n",
            "2454\n",
            "2455\n",
            "2456\n",
            "2457\n",
            "2458\n",
            "2459\n",
            "2460\n",
            "2461\n",
            "2462\n",
            "2463\n",
            "2464\n",
            "2465\n",
            "2466\n",
            "2467\n",
            "2468\n",
            "2469\n",
            "2470\n",
            "2471\n",
            "2472\n",
            "2473\n",
            "2474\n",
            "2475\n",
            "2476\n",
            "2477\n",
            "2478\n",
            "2479\n",
            "2480\n",
            "2481\n",
            "2482\n",
            "2483\n",
            "2484\n",
            "2485\n",
            "2486\n",
            "2487\n",
            "2488\n",
            "2489\n",
            "2490\n",
            "2491\n",
            "2492\n",
            "2493\n",
            "2494\n",
            "2495\n",
            "2496\n",
            "2497\n",
            "2498\n",
            "2499\n",
            "2500\n",
            "2501\n",
            "2502\n",
            "2503\n",
            "2504\n",
            "2505\n",
            "2506\n",
            "2507\n",
            "2508\n",
            "2509\n",
            "2510\n",
            "2511\n",
            "2512\n",
            "2513\n",
            "2514\n",
            "2515\n",
            "2516\n",
            "2517\n",
            "2518\n",
            "2519\n",
            "2520\n",
            "2521\n",
            "2522\n",
            "2523\n",
            "2524\n",
            "2525\n",
            "2526\n",
            "2527\n",
            "2528\n",
            "2529\n",
            "2530\n",
            "2531\n",
            "2532\n",
            "2533\n",
            "2534\n",
            "2535\n",
            "2536\n",
            "2537\n",
            "2538\n",
            "2539\n",
            "2540\n",
            "2541\n",
            "2542\n",
            "2543\n",
            "2544\n",
            "2545\n",
            "2546\n",
            "2547\n",
            "2548\n",
            "2549\n",
            "2550\n",
            "2551\n",
            "2552\n",
            "2553\n",
            "2554\n",
            "2555\n",
            "2556\n",
            "2557\n",
            "2558\n",
            "2559\n",
            "2560\n",
            "2561\n",
            "2562\n",
            "2563\n",
            "2564\n",
            "2565\n",
            "2566\n",
            "2567\n",
            "2568\n",
            "2569\n",
            "2570\n",
            "2571\n",
            "2572\n",
            "2573\n",
            "2574\n",
            "2575\n",
            "2576\n",
            "2577\n",
            "2578\n",
            "2579\n",
            "2580\n",
            "2581\n",
            "2582\n",
            "2583\n",
            "2584\n",
            "2585\n",
            "2586\n",
            "2587\n",
            "2588\n",
            "2589\n",
            "2590\n",
            "2591\n",
            "2592\n",
            "2593\n",
            "2594\n",
            "2595\n",
            "2596\n",
            "2597\n",
            "2598\n",
            "2599\n",
            "2600\n",
            "2601\n",
            "2602\n",
            "2603\n",
            "2604\n",
            "2605\n",
            "2606\n",
            "2607\n",
            "2608\n",
            "2609\n",
            "2610\n",
            "2611\n",
            "2612\n",
            "2613\n",
            "2614\n",
            "2615\n",
            "2616\n",
            "2617\n",
            "2618\n",
            "2619\n",
            "2620\n",
            "2621\n",
            "2622\n",
            "2623\n",
            "2624\n",
            "2625\n",
            "2626\n",
            "2627\n",
            "2628\n",
            "2629\n",
            "2630\n",
            "2631\n",
            "2632\n",
            "2633\n",
            "2634\n",
            "2635\n",
            "2636\n",
            "2637\n",
            "2638\n",
            "2639\n",
            "2640\n",
            "2641\n",
            "2642\n",
            "2643\n",
            "2644\n",
            "2645\n",
            "2646\n",
            "2647\n",
            "2648\n",
            "2649\n",
            "2650\n",
            "2651\n",
            "2652\n",
            "2653\n",
            "2654\n",
            "2655\n",
            "2656\n",
            "2657\n",
            "2658\n",
            "2659\n",
            "2660\n",
            "2661\n",
            "2662\n",
            "2663\n",
            "2664\n",
            "2665\n",
            "2666\n",
            "2667\n",
            "2668\n",
            "2669\n",
            "2670\n",
            "2671\n",
            "2672\n",
            "2673\n",
            "2674\n",
            "2675\n",
            "2676\n",
            "2677\n",
            "2678\n",
            "2679\n",
            "2680\n",
            "2681\n",
            "2682\n",
            "2683\n",
            "2684\n",
            "2685\n",
            "2686\n",
            "2687\n",
            "2688\n",
            "2689\n",
            "2690\n",
            "2691\n",
            "2692\n",
            "2693\n",
            "2694\n",
            "2695\n",
            "2696\n",
            "2697\n",
            "2698\n",
            "2699\n",
            "2700\n",
            "2701\n",
            "2702\n",
            "2703\n",
            "2704\n",
            "2705\n",
            "2706\n",
            "2707\n",
            "2708\n",
            "2709\n",
            "2710\n",
            "2711\n",
            "2712\n",
            "2713\n",
            "2714\n",
            "2715\n",
            "2716\n",
            "2717\n",
            "2718\n",
            "2719\n",
            "2720\n",
            "2721\n",
            "2722\n",
            "2723\n",
            "2724\n",
            "2725\n",
            "2726\n",
            "2727\n",
            "2728\n",
            "2729\n",
            "2730\n",
            "2731\n",
            "2732\n",
            "2733\n",
            "2734\n",
            "2735\n",
            "2736\n",
            "2737\n",
            "2738\n",
            "2739\n",
            "2740\n",
            "2741\n",
            "2742\n",
            "2743\n",
            "2744\n",
            "2745\n",
            "2746\n",
            "2747\n",
            "2748\n",
            "2749\n",
            "2750\n",
            "2751\n",
            "2752\n",
            "2753\n",
            "2754\n",
            "2755\n",
            "2756\n",
            "2757\n",
            "2758\n",
            "2759\n",
            "2760\n",
            "2761\n",
            "2762\n",
            "2763\n",
            "2764\n",
            "2765\n",
            "2766\n",
            "2767\n",
            "2768\n",
            "2769\n",
            "2770\n",
            "2771\n",
            "2772\n",
            "2773\n",
            "2774\n",
            "2775\n",
            "2776\n",
            "2777\n",
            "2778\n",
            "2779\n",
            "2780\n",
            "2781\n",
            "2782\n",
            "2783\n",
            "2784\n",
            "2785\n",
            "2786\n",
            "2787\n",
            "2788\n",
            "2789\n",
            "2790\n",
            "2791\n",
            "2792\n",
            "2793\n",
            "2794\n",
            "2795\n",
            "2796\n",
            "2797\n",
            "2798\n",
            "2799\n",
            "2800\n",
            "2801\n",
            "2802\n",
            "2803\n",
            "2804\n",
            "2805\n",
            "2806\n",
            "2807\n",
            "2808\n",
            "2809\n",
            "2810\n",
            "2811\n",
            "2812\n",
            "2813\n",
            "2814\n",
            "2815\n",
            "2816\n",
            "2817\n",
            "2818\n",
            "2819\n",
            "2820\n",
            "2821\n",
            "2822\n",
            "2823\n",
            "2824\n",
            "2825\n",
            "2826\n",
            "2827\n",
            "2828\n",
            "2829\n",
            "2830\n",
            "2831\n",
            "2832\n",
            "2833\n",
            "2834\n",
            "2835\n",
            "2836\n",
            "2837\n",
            "2838\n",
            "2839\n",
            "2840\n",
            "2841\n",
            "2842\n",
            "2843\n",
            "2844\n",
            "2845\n",
            "2846\n",
            "2847\n",
            "2848\n",
            "2849\n",
            "2850\n",
            "2851\n",
            "2852\n",
            "2853\n",
            "2854\n",
            "2855\n",
            "2856\n",
            "2857\n",
            "2858\n",
            "2859\n",
            "2860\n",
            "2861\n",
            "2862\n",
            "2863\n",
            "2864\n",
            "2865\n",
            "2866\n",
            "2867\n",
            "2868\n",
            "2869\n",
            "2870\n",
            "2871\n",
            "2872\n",
            "2873\n",
            "2874\n",
            "2875\n",
            "2876\n",
            "2877\n",
            "2878\n",
            "2879\n",
            "2880\n",
            "2881\n",
            "2882\n",
            "2883\n",
            "2884\n",
            "2885\n",
            "2886\n",
            "2887\n",
            "2888\n",
            "2889\n",
            "2890\n",
            "2891\n",
            "2892\n",
            "2893\n",
            "2894\n",
            "2895\n",
            "2896\n",
            "2897\n",
            "2898\n",
            "2899\n",
            "2900\n",
            "2901\n",
            "2902\n",
            "2903\n",
            "2904\n",
            "2905\n",
            "2906\n",
            "2907\n",
            "2908\n",
            "2909\n",
            "2910\n",
            "2911\n",
            "2912\n",
            "2913\n",
            "2914\n",
            "2915\n",
            "2916\n",
            "2917\n",
            "2918\n",
            "2919\n",
            "2920\n",
            "2921\n",
            "2922\n",
            "2923\n",
            "2924\n",
            "2925\n",
            "2926\n",
            "2927\n",
            "2928\n",
            "2929\n",
            "2930\n",
            "2931\n",
            "2932\n",
            "2933\n",
            "2934\n",
            "2935\n",
            "2936\n",
            "2937\n",
            "2938\n",
            "2939\n",
            "2940\n",
            "2941\n",
            "2942\n",
            "2943\n",
            "2944\n",
            "2945\n",
            "2946\n",
            "2947\n",
            "2948\n",
            "2949\n",
            "2950\n",
            "2951\n",
            "2952\n",
            "2953\n",
            "2954\n",
            "2955\n",
            "2956\n",
            "2957\n",
            "2958\n",
            "2959\n",
            "2960\n",
            "2961\n",
            "2962\n",
            "2963\n",
            "2964\n",
            "2965\n",
            "2966\n",
            "2967\n",
            "2968\n",
            "2969\n",
            "2970\n",
            "2971\n",
            "2972\n",
            "2973\n",
            "2974\n",
            "2975\n",
            "2976\n",
            "2977\n",
            "2978\n",
            "2979\n",
            "2980\n",
            "2981\n",
            "2982\n",
            "2983\n",
            "2984\n",
            "2985\n",
            "2986\n",
            "2987\n",
            "2988\n",
            "2989\n",
            "2990\n",
            "2991\n",
            "2992\n",
            "2993\n",
            "2994\n",
            "2995\n",
            "2996\n",
            "2997\n",
            "2998\n",
            "2999\n",
            "3000\n",
            "3001\n",
            "3002\n",
            "3003\n",
            "3004\n",
            "3005\n",
            "3006\n",
            "3007\n",
            "3008\n",
            "3009\n",
            "3010\n",
            "3011\n",
            "3012\n",
            "3013\n",
            "3014\n",
            "3015\n",
            "3016\n",
            "3017\n",
            "3018\n",
            "3019\n",
            "3020\n",
            "3021\n",
            "3022\n",
            "3023\n",
            "3024\n",
            "3025\n",
            "3026\n",
            "3027\n",
            "3028\n",
            "3029\n",
            "3030\n",
            "3031\n",
            "3032\n",
            "3033\n",
            "3034\n",
            "3035\n",
            "3036\n",
            "3037\n",
            "3038\n",
            "3039\n",
            "3040\n",
            "3041\n",
            "3042\n",
            "3043\n",
            "3044\n",
            "3045\n",
            "3046\n",
            "3047\n",
            "3048\n",
            "3049\n",
            "3050\n",
            "3051\n",
            "3052\n",
            "3053\n",
            "3054\n",
            "3055\n",
            "3056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-169e80fe95ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m       \u001b[0mencoded_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mbert_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0mextended_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10000.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mwords_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m         return F.embedding(\n\u001b[1;32m    117\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCf5WIqtngNW",
        "outputId": "f0831cf3-72a9-4c64-8cc1-d7ac1e5edc8a"
      },
      "source": [
        "#bert_embedding = bert_embedding[1:]\n",
        "print(bert_embedding[1:].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvYZuYq50e-1",
        "outputId": "6a6b0094-67f6-42ba-9ff2-cc6f2f32736d"
      },
      "source": [
        "if len(indexed_tokens) == 0:\n",
        "  print(indexed_tokens)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Blah []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osHNzSVmkS6S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8r435CYZkoVk",
        "outputId": "92ca584f-8159-4c18-c59d-71eb217cca93"
      },
      "source": [
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "with open('embeddings_dict.pickle', 'rb') as handle:\n",
        "    embb = pickle.load(handle)\n",
        "\n",
        "embb_keys = list(embb.keys())\n",
        "\n",
        "emp_emb = np.empty([len(embb_keys), 768])\n",
        "\n",
        "for i in range(len(embb_keys)):\n",
        "  key_val = embb[embb_keys[i]]\n",
        "  emp_emb[i] = key_val\n",
        "\n",
        "print(emp_emb.shape)\n",
        "\n",
        "x_torch = torch.from_numpy(emp_emb)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3674, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssd9_fDRB7Kk",
        "outputId": "2e587ad1-0f28-474c-9fb2-39078b712a6d"
      },
      "source": [
        "print(x_torch.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3674, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTs_1-aBzMdD",
        "outputId": "c949f52b-400c-48bd-8af5-82967dc985cc"
      },
      "source": [
        "emp_arr1 = np.empty([4, 4])\n",
        "emp_arr1[0] = np.array([123000,100,100,100])\n",
        "\n",
        "print(emp_arr1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.23000000e+005 1.00000000e+002 1.00000000e+002 1.00000000e+002]\n",
            " [1.03977794e-312 1.01855798e-312 1.23075756e-312 1.10343781e-312]\n",
            " [1.03977794e-312 9.76118064e-313 1.08221785e-312 1.18831764e-312]\n",
            " [1.06099790e-312 1.74490058e-015 4.64720292e-023 5.07136406e-018]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GrZUFnzuh-LK",
        "outputId": "5c1c99bf-3655-40ff-9ed3-56bb83bb2972"
      },
      "source": [
        "cap"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp5Smi7pf44E",
        "outputId": "7e39d1eb-1893-44de-a596-28083a10d05a"
      },
      "source": [
        "x = x.to(\"cpu\")\n",
        "x_np = x.data.numpy()\n",
        "\n",
        "print(x_np.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "gcw_aITbOmzJ",
        "outputId": "0045cc25-9b1a-408c-cb2c-dc75a1a136bc"
      },
      "source": [
        "print(split_cap)\n",
        "print(tokenized_cap)\n",
        "print(bert_embedding[0].shape)\n",
        "\n",
        "i1 = bert_embedding[0]\n",
        "i1 = i1.reshape(1,768)\n",
        "\n",
        "i1 = i1.to_numpy()\n",
        "print(i1.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['时代']\n",
            "['时', '代']\n",
            "torch.Size([768])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-d53217c7844a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mi1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mi1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'to_numpy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuCAh7QtOXPR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}